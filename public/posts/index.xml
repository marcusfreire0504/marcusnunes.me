<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
	<channel>
		<title>Marcus Nunes&#39; Blog</title>
		<link>https://marcusnunes.me/posts/</link>
		<description>Recent content on Marcus Nunes&#39; Blog</description>
		<generator>Hugo -- gohugo.io</generator>
		<language>pt-br</language>
		<copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright>
		<lastBuildDate>Fri, 02 Feb 2018 12:00:00 -0300</lastBuildDate>
		<atom:link href="https://marcusnunes.me/posts/index.xml" rel="self" type="application/rss+xml" />
		
		<item>
			<title>Livros de Ciência de Dados Gratuitos na Amazon</title>
			<link>https://marcusnunes.me/posts/livros-gratuitos-da-amazon/</link>
			<pubDate>Thu, 09 Apr 2020 14:50:00 -0300</pubDate>
			
			<guid>https://marcusnunes.me/posts/livros-gratuitos-da-amazon/</guid>
			<description>Recebi esta dica no grupo de telegram Ciência de Dados RN. A Amazon está disponibilizando gratuitamente 28 de livros sobre Ciência de Dados, Big Data e programação em suas versões para Kindle.
Assim como no casos dos livros gratuitos da Springer, basta entrar nos link fornecidos abaixo e mandar o livro ser baixado. Dá para enviar diretamente para o Kindle ou ler através do aplicativo da Amazon em outras plataformas.</description>
			<content type="html"><![CDATA[<p>Recebi esta dica no grupo de telegram Ciência de Dados RN. A Amazon está disponibilizando gratuitamente 28 de livros sobre Ciência de Dados, Big Data e programação em suas versões para Kindle.</p>

<p><img src="/images/amazon-free.png" alt="The Everyday Life of an Algorithm" /></p>

<p>Assim como no casos dos <a href="https://marcusnunes.me/posts/livros-gratuitos-da-springer/" target="_blank">livros gratuitos da Springer</a>, basta entrar nos link fornecidos abaixo e mandar o livro ser baixado. Dá para enviar diretamente para o Kindle ou ler através do aplicativo da Amazon em outras plataformas.</p>

<ul>
<li><a href="https://amzn.to/2y9g4KN" target="_blank">Automated Machine Learning</a></li>
<li><a href="https://amzn.to/2QIBjJQ" target="_blank">Big Data Analytics Options on AWS</a></li>
<li><a href="https://amzn.to/2xZ5aH8" target="_blank">Big Data for Executives and Market Professionals (English Edition)</a></li>
<li><a href="https://amzn.to/2UeuRMw" target="_blank">Big Data in Context</a></li>
<li><a href="https://amzn.to/2wzaFvW" target="_blank">Big Data Now 2012 Edition</a></li>
<li><a href="https://amzn.to/2UxIzt5" target="_blank">Big Data Now Current Perspectives from O&rsquo;Reilly Radar</a></li>
<li><a href="https://amzn.to/2wq7rej" target="_blank">Bioimage Data Analysis Workflows</a></li>
<li><a href="https://amzn.to/3bi2u5Q" target="_blank">Building Data Science Teams (English Edition)</a></li>
<li><a href="https://amzn.to/2QGpaF0" target="_blank">Data &amp; Analytics Reading Sampler Excerpts (English Edition)</a></li>
<li><a href="https://amzn.to/3bsluhZ" target="_blank">Data Manipulation (R Fundamentals Book 2)</a></li>
<li><a href="https://amzn.to/3bpzb1q" target="_blank">Fundamentals of Clinical Data Science</a></li>
<li><a href="https://amzn.to/2WFYb0g" target="_blank">High-Performance Modelling and Simulation for Big Data Applications</a></li>
<li><a href="https://amzn.to/2xZ4Hom" target="_blank">How Data Science Is Transforming Health Care (English Edition)</a></li>
<li><a href="https://amzn.to/2WHgGkM" target="_blank">Introduction to Data Analysis in Qualitative Research (English Edition)</a></li>
<li><a href="https://amzn.to/2UCBQhz" target="_blank">Market Segmentation Analysis</a></li>
<li><a href="https://amzn.to/2WFl6Zv" target="_blank">New Horizons for a Data-Driven Economy</a></li>
<li><a href="https://amzn.to/2QIaEN6" target="_blank">Planning for Big Data (English Edition)</a></li>
<li><a href="https://amzn.to/2UxFjhg" target="_blank">Programming for Computations - Python</a></li>
<li><a href="https://amzn.to/2QYeC4v" target="_blank">Python 3 Manuscripts in 1 book</a></li>
<li><a href="https://amzn.to/3dpzQSd" target="_blank">Python for Beginners</a></li>
<li><a href="https://amzn.to/2JcoL9h" target="_blank">Python Programming 2 book in 1</a></li>
<li><a href="https://amzn.to/3ai6Mds" target="_blank">Python Programming For Beginners</a></li>
<li><a href="https://amzn.to/2wD2nmP" target="_blank">Real-Time Big Data Analytics Emerging Architecture</a></li>
<li><a href="https://amzn.to/2QKEyAp" target="_blank">Redes Neurais Artificiais em 45 Minutos - 2a edição</a></li>
<li><a href="https://amzn.to/3dtlsZi" target="_blank">Social Networks with Rich Edge Semantics (Open Access) (Chapman &amp; Hall/CRC Data Mining and Knowledge</a></li>
<li><a href="https://amzn.to/3bkQcK7" target="_blank">The Culture of Big Data (English Edition)</a></li>
<li><a href="https://amzn.to/3dBCRz4" target="_blank">The Everyday Life of an Algorithm</a></li>
<li><a href="https://amzn.to/2wzyuno" target="_blank">What Is Data Science?</a></li>
</ul>
]]></content>
		</item>
		
		<item>
			<title>Livros Gratuitos da Springer</title>
			<link>https://marcusnunes.me/posts/livros-gratuitos-da-springer/</link>
			<pubDate>Fri, 03 Apr 2020 06:57:00 -0300</pubDate>
			
			<guid>https://marcusnunes.me/posts/livros-gratuitos-da-springer/</guid>
			<description>Acabei de saber na lista de emails da Associação Brasileira de Estatística que a editora Springer disponibilizou mais de 500 livros gratuitamente. Basta visitar o site e baixar, sem sequer necessitar fazer cadastro. A lista completa de livros está nesta planilha do Excel, mas abaixo eu coloquei a listagem de livros de Matemática e Estatística, colocados em ordem alfabética do título. Aproveitem.
 A Beginner&amp;rsquo;s Guide to R - Alain Zuur, Elena N.</description>
			<content type="html"><![CDATA[<p>Acabei de saber na lista de emails da Associação Brasileira de Estatística que a editora Springer disponibilizou mais de 500 livros gratuitamente. Basta visitar o site e baixar, sem sequer necessitar fazer cadastro. A lista completa de livros está <a href="/images/FreeEnglishTextbooks.xlsx">nesta planilha do Excel</a>, mas abaixo eu coloquei a listagem de livros de Matemática e Estatística, colocados em ordem alfabética do título. Aproveitem.</p>

<ul>
<li><a href="http://link.springer.com/openurl?genre=book&amp;isbn=978-0-387-93837-0" target="_blank">A Beginner&rsquo;s Guide to R</a> - Alain Zuur, Elena N. Ieno, Erik Meesters</li>
<li><a href="http://link.springer.com/openurl?genre=book&amp;isbn=978-1-84628-168-6" target="_blank">A Modern Introduction to Probability and Statistics</a> - F.M. Dekking, C. Kraaikamp, H.P. Lopuhaä, L.E. Meester</li>
<li><a href="http://link.springer.com/openurl?genre=book&amp;isbn=978-3-662-49887-3" target="_blank">A Primer on Scientific Programming with Python</a> - Hans Petter Langtangen</li>
<li><a href="http://link.springer.com/openurl?genre=book&amp;isbn=978-3-030-02604-2" target="_blank">A Pythagorean Introduction to Number Theory</a> - Ramin Takloo-Bighash</li>
<li><a href="http://link.springer.com/openurl?genre=book&amp;isbn=978-3-319-77649-1" target="_blank">Abstract Algebra</a> - Gregory T. Lee</li>
<li><a href="http://link.springer.com/openurl?genre=book&amp;isbn=978-1-4613-0041-0" target="_blank">Algebra</a> - Serge Lang</li>
<li><a href="http://link.springer.com/openurl?genre=book&amp;isbn=978-0-387-21736-9" target="_blank">All of Statistics</a> - Larry Wasserman</li>
<li><a href="http://link.springer.com/openurl?genre=book&amp;isbn=978-1-4614-7138-7" target="_blank">An Introduction to Statistical Learning</a> - Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani</li>
<li><a href="http://link.springer.com/openurl?genre=book&amp;isbn=978-3-319-91041-3" target="_blank">Applied Linear Algebra</a> - Peter J. Olver, Chehrzad Shakiban</li>
<li><a href="http://link.springer.com/openurl?genre=book&amp;isbn=978-3-662-45171-7" target="_blank">Applied Multivariate Statistical Analysis</a> - Wolfgang Karl Härdle, Léopold Simar</li>
<li><a href="http://link.springer.com/openurl?genre=book&amp;isbn=978-3-319-12493-3" target="_blank">Applied Partial Differential Equations</a> - J. David Logan</li>
<li><a href="http://link.springer.com/openurl?genre=book&amp;isbn=978-1-4614-6849-3" target="_blank">Applied Predictive Modeling</a> - Max Kuhn, Kjell Johnson</li>
<li><a href="http://link.springer.com/openurl?genre=book&amp;isbn=978-3-662-54486-0" target="_blank">Applied Quantitative Finance</a> - Wolfgang Karl Härdle, Cathy Yi-Hsuan Chen, Ludger Overbeck</li>
<li><a href="http://link.springer.com/openurl?genre=book&amp;isbn=978-1-4419-0925-1" target="_blank">Bayesian and Frequentist Regression Methods</a> - Jon Wakefield</li>
<li><a href="http://link.springer.com/openurl?genre=book&amp;isbn=978-1-4614-8687-9" target="_blank">Bayesian Essentials with R</a> - Jean-Michel Marin, Christian P. Robert</li>
<li><a href="http://link.springer.com/openurl?genre=book&amp;isbn=978-3-319-31089-3" target="_blank">Brownian Motion, Martingales, and Stochastic Calculus </a> - Jean-François Le Gall</li>
<li><a href="http://link.springer.com/openurl?genre=book&amp;isbn=978-3-319-32185-1" target="_blank">Business Statistics for Competitive Advantage with Excel 2016 </a> - Cynthia Fraser <strong>(este não está gratuito)</strong></li>
<li><a href="http://link.springer.com/openurl?genre=book&amp;isbn=978-1-4614-7946-8" target="_blank">Calculus With Applications</a> - Peter D. Lax, Maria Shea Terrell</li>
<li><a href="http://link.springer.com/openurl?genre=book&amp;isbn=978-1-4939-1194-3" target="_blank">Classical Fourier Analysis</a> - Loukas Grafakos</li>
<li><a href="http://link.springer.com/openurl?genre=book&amp;isbn=978-1-4419-7288-0" target="_blank">Complex Analysis</a> - Joseph Bak, Donald J. Newman</li>
<li><a href="http://link.springer.com/openurl?genre=book&amp;isbn=978-3-319-52250-0" target="_blank">Design and Analysis of Experiments</a> - Angela Dean, Daniel Voss, Danel Draguljić</li>
<li><a href="http://link.springer.com/openurl?genre=book&amp;isbn=978-1-4612-4360-1" target="_blank">Differential Equations and Their Applications</a> - Martin Braun</li>
<li><a href="http://link.springer.com/openurl?genre=book&amp;isbn=978-0-387-21777-2" target="_blank">Discrete Mathematics</a> - László Lovász, József Pelikán, Katalin Vesztergombi</li>
<li><a href="http://link.springer.com/openurl?genre=book&amp;isbn=978-1-4614-6271-2" target="_blank">Elementary Analysis</a> - Kenneth A. Ross</li>
<li><a href="http://link.springer.com/openurl?genre=book&amp;isbn=978-3-319-18539-2" target="_blank">Fundamentals of Clinical Trials</a> - Lawrence M. Friedman, Curt D. Furberg, David L. DeMets, David M. Reboussin, Christopher B. Granger</li>
<li><a href="http://link.springer.com/openurl?genre=book&amp;isbn=978-3-319-48936-0" target="_blank">Introduction to Partial Differential Equations</a> - David Borthwick</li>
<li><a href="http://link.springer.com/openurl?genre=book&amp;isbn=978-3-319-02099-0" target="_blank">Introduction to Partial Differential Equations</a> - Peter J. Olver</li>
<li><a href="http://link.springer.com/openurl?genre=book&amp;isbn=978-1-4419-9982-5" target="_blank">Introduction to Smooth Manifolds</a> - John Lee</li>
<li><a href="http://link.springer.com/openurl?genre=book&amp;isbn=978-3-319-46162-5" target="_blank">Introduction to Statistics and Data Analysis </a> - Christian Heumann, Michael Schomaker,  Shalabh</li>
<li><a href="http://link.springer.com/openurl?genre=book&amp;isbn=978-3-319-29854-2" target="_blank">Introduction to Time Series and Forecasting</a> - Peter J. Brockwell, Richard A. Davis</li>
<li><a href="http://link.springer.com/openurl?genre=book&amp;isbn=978-0-387-79054-1" target="_blank">Introductory Statistics with R</a> - Peter Dalgaard</li>
<li><a href="http://link.springer.com/openurl?genre=book&amp;isbn=978-0-387-88698-5" target="_blank">Introductory Time Series with R</a> - Paul S.P. Cowpertwait, Andrew V. Metcalfe</li>
<li><a href="http://link.springer.com/openurl?genre=book&amp;isbn=978-3-319-24346-7" target="_blank">Linear Algebra</a> - Jörg Liesen, Volker Mehrmann</li>
<li><a href="http://link.springer.com/openurl?genre=book&amp;isbn=978-3-319-11080-6" target="_blank">Linear Algebra Done Right</a> - Sheldon Axler</li>
<li><a href="http://link.springer.com/openurl?genre=book&amp;isbn=978-3-319-23042-9" target="_blank">Methods of Mathematical Modelling</a> - Thomas Witelski, Mark Bowen</li>
<li><a href="http://link.springer.com/openurl?genre=book&amp;isbn=978-3-319-59731-7" target="_blank">Modeling Life</a> - Alan Garfinkel, Jane Shevtsov, Yina Guo</li>
<li><a href="http://link.springer.com/openurl?genre=book&amp;isbn=978-1-4471-6419-7" target="_blank">Multivariate Calculus and Geometry</a> - Seán Dineen</li>
<li><a href="http://link.springer.com/openurl?genre=book&amp;isbn=978-0-387-40065-5" target="_blank">Numerical Optimization</a> - Jorge Nocedal, Stephen Wright</li>
<li><a href="http://link.springer.com/openurl?genre=book&amp;isbn=978-1-4614-3618-8" target="_blank">Ordinary Differential Equations</a> - William A. Adkins, Mark G. Davidson</li>
<li><a href="http://link.springer.com/openurl?genre=book&amp;isbn=978-1-4614-4809-9" target="_blank">Partial Differential Equations</a> - Jürgen Jost</li>
<li><a href="http://link.springer.com/openurl?genre=book&amp;isbn=978-1-4612-4374-8" target="_blank">Probability</a> - Jim Pitman</li>
<li><a href="http://link.springer.com/openurl?genre=book&amp;isbn=978-1-4471-5201-9" target="_blank">Probability Theory</a> - Alexandr A. Borovkov</li>
<li><a href="http://link.springer.com/openurl?genre=book&amp;isbn=978-1-4471-5361-0" target="_blank">Probability Theory</a> - Achim Klenke</li>
<li><a href="http://link.springer.com/openurl?genre=book&amp;isbn=978-3-662-57265-8" target="_blank">Proofs from THE BOOK</a> - Martin Aigner, Günter M. Ziegler</li>
<li><a href="http://link.springer.com/openurl?genre=book&amp;isbn=978-1-4614-7116-5" target="_blank">Quantum Theory for Mathematicians</a> - Brian C. Hall</li>
<li><a href="http://link.springer.com/openurl?genre=book&amp;isbn=978-1-4419-9479-0" target="_blank">Reading, Writing, and Proving</a> - Ulrich Daepp, Pamela Gorkin</li>
<li><a href="http://link.springer.com/openurl?genre=book&amp;isbn=978-1-4939-2766-1" target="_blank">Real Analysis</a> - Miklós Laczkovich, Vera T. Sós</li>
<li><a href="http://link.springer.com/openurl?genre=book&amp;isbn=978-3-319-19425-7" target="_blank">Regression Modeling Strategies</a> - Frank E. Harrell , Jr.</li>
<li><a href="http://link.springer.com/openurl?genre=book&amp;isbn=978-1-4612-0979-9" target="_blank">Representation Theory</a> - William Fulton, Joe Harris</li>
<li><a href="http://link.springer.com/openurl?genre=book&amp;isbn=978-1-4939-2122-5" target="_blank">Statistical Analysis and Data Display</a> - Richard M. Heiberger, Burt Holland</li>
<li><a href="http://link.springer.com/openurl?genre=book&amp;isbn=978-3-319-44048-4" target="_blank">Statistical Learning from a Regression Perspective</a> - Richard A. Berk</li>
<li><a href="http://link.springer.com/openurl?genre=book&amp;isbn=978-1-4939-2614-5" target="_blank">Statistics and Data Analysis for Financial Engineering</a> - David Ruppert, David S. Matteson</li>
<li><a href="http://link.springer.com/openurl?genre=book&amp;isbn=978-1-4419-6646-9" target="_blank">Survival Analysis</a> - David G. Kleinbaum, Mitchel Klein</li>
<li><a href="http://link.springer.com/openurl?genre=book&amp;isbn=978-0-387-84858-7" target="_blank">The Elements of Statistical Learning</a> - Trevor Hastie, Robert Tibshirani, Jerome Friedman</li>
<li><a href="http://link.springer.com/openurl?genre=book&amp;isbn=978-0-387-75959-3" target="_blank">Time Series Analysis</a> - Jonathan D. Cryer, Kung-Sik Chan</li>
<li><a href="http://link.springer.com/openurl?genre=book&amp;isbn=978-1-4939-2712-8" target="_blank">Understanding Analysis</a> - Stephen Abbott</li>
<li><a href="http://link.springer.com/openurl?genre=book&amp;isbn=978-1-4614-6227-9" target="_blank">Understanding Statistics Using R</a> - Randall Schumacker, Sara Tomek</li>
</ul>
]]></content>
		</item>
		
		<item>
			<title>Análise dos Dados de Coronavírus do Hospital Albert Einstein: 90% de Acurácia e Porque isso é Ruim</title>
			<link>https://marcusnunes.me/posts/covid-19-einstein-90-acuracia/</link>
			<pubDate>Tue, 31 Mar 2020 12:56:00 -0300</pubDate>
			
			<guid>https://marcusnunes.me/posts/covid-19-einstein-90-acuracia/</guid>
			<description>Introdução Recentemente, o Hospital Albert Einstein compartilhou um conjunto de dados com observações de 5644 pacientes. Os dados possuem 111 características relacionadas a exames médicos, como sangue, urina e muito mais. A tarefa que me propus a resolver foi classificar os pacientes testados em dois grupos, aqueles que posssuem e aqueles que não possuem coronavírus. Farei isso a partir a partir dos resultados dos exames laboratoriais, sem utilizar informação alguma sobre o diagnóstico realizado em relação à contaminação do vírus.</description>
			<content type="html"><![CDATA[
<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>


<div id="introdução" class="section level1">
<h1>Introdução</h1>
<p>Recentemente, o Hospital Albert Einstein compartilhou um conjunto de dados com observações de 5644 pacientes. Os dados possuem 111 características relacionadas a exames médicos, como sangue, urina e muito mais. A tarefa que me propus a resolver foi classificar os pacientes testados em dois grupos, aqueles que posssuem e aqueles que não possuem coronavírus. Farei isso a partir a partir dos resultados dos exames laboratoriais, sem utilizar informação alguma sobre o diagnóstico realizado em relação à contaminação do vírus.</p>
<p>Caso a técnica utilizada neste posto não tenha ficado muito clara, recomendo a leitura do meu post <a href="https://marcusnunes.me/posts/primeiro-projeto-de-data-science/">Tutorial: Como Fazer o Seu Primeiro Projeto de Data Science</a>, que mostra com mais detalhes como ajustar o modelo random forest a um conjunto de dados.</p>
<p>Os dados utilizados aqui, bem como o código utilizado nesta análise, estão disponíveis em meu <a href="https://github.com/mnunes/einstein-covid-19">github</a>.</p>
</div>
<div id="análise-exploratória-dos-dados" class="section level1">
<h1>Análise Exploratória dos Dados</h1>
<p>O primeiro passo em qualquer análise de dados é a análise exploratória. É possível gerar ideias e insights apenas olhando para os dados plotados. Vou começar minha análise exploratória carregando alguns pacotes no R:</p>
<pre class="r"><code>library(tidyverse)
theme_set(theme_bw())
library(naniar)
library(readxl)
library(caret)
library(reshape2)
library(GGally)
library(mice)</code></pre>
<p>Com os pacotes carregados, vou ler os dados e realizar pequenas correções neles. Cada passo a seguir foi explicado com um comentário dentro do próprio código.</p>
<pre class="r"><code>dataset &lt;- read_excel(path = &quot;data/dataset.xlsx&quot;)

# fix column names

names(dataset) &lt;- make.names(names(dataset), unique = TRUE)

#################################
### exploratory data analysis ###
#################################

# first look at the dataset

glimpse(dataset)</code></pre>
<pre><code>## Rows: 5,644
## Columns: 111
## $ Patient.ID                                            &lt;chr&gt; &quot;44477f75e8169d…
## $ Patient.age.quantile                                  &lt;dbl&gt; 13, 17, 8, 5, 1…
## $ SARS.Cov.2.exam.result                                &lt;chr&gt; &quot;negative&quot;, &quot;ne…
## $ Patient.addmited.to.regular.ward..1.yes..0.no.        &lt;dbl&gt; 0, 0, 0, 0, 0, …
## $ Patient.addmited.to.semi.intensive.unit..1.yes..0.no. &lt;dbl&gt; 0, 0, 0, 0, 0, …
## $ Patient.addmited.to.intensive.care.unit..1.yes..0.no. &lt;dbl&gt; 0, 0, 0, 0, 0, …
## $ Hematocrit                                            &lt;dbl&gt; NA, 0.2365154, …
## $ Hemoglobin                                            &lt;dbl&gt; NA, -0.02234027…
## $ Platelets                                             &lt;dbl&gt; NA, -0.51741302…
## $ Mean.platelet.volume                                  &lt;dbl&gt; NA, 0.01067657,…
## $ Red.blood.Cells                                       &lt;dbl&gt; NA, 0.1020042, …
## $ Lymphocytes                                           &lt;dbl&gt; NA, 0.318365753…
## $ Mean.corpuscular.hemoglobin.concentration..MCHC.      &lt;dbl&gt; NA, -0.9507903,…
## $ Leukocytes                                            &lt;dbl&gt; NA, -0.09461035…
## $ Basophils                                             &lt;dbl&gt; NA, -0.22376651…
## $ Mean.corpuscular.hemoglobin..MCH.                     &lt;dbl&gt; NA, -0.29226932…
## $ Eosinophils                                           &lt;dbl&gt; NA, 1.4821582, …
## $ Mean.corpuscular.volume..MCV.                         &lt;dbl&gt; NA, 0.1661924, …
## $ Monocytes                                             &lt;dbl&gt; NA, 0.35754666,…
## $ Red.blood.cell.distribution.width..RDW.               &lt;dbl&gt; NA, -0.6250727,…
## $ Serum.Glucose                                         &lt;dbl&gt; NA, -0.1406481,…
## $ Respiratory.Syncytial.Virus                           &lt;chr&gt; NA, &quot;not_detect…
## $ Influenza.A                                           &lt;chr&gt; NA, &quot;not_detect…
## $ Influenza.B                                           &lt;chr&gt; NA, &quot;not_detect…
## $ Parainfluenza.1                                       &lt;chr&gt; NA, &quot;not_detect…
## $ CoronavirusNL63                                       &lt;chr&gt; NA, &quot;not_detect…
## $ Rhinovirus.Enterovirus                                &lt;chr&gt; NA, &quot;detected&quot;,…
## $ Mycoplasma.pneumoniae                                 &lt;lgl&gt; NA, NA, NA, NA,…
## $ Coronavirus.HKU1                                      &lt;chr&gt; NA, &quot;not_detect…
## $ Parainfluenza.3                                       &lt;chr&gt; NA, &quot;not_detect…
## $ Chlamydophila.pneumoniae                              &lt;chr&gt; NA, &quot;not_detect…
## $ Adenovirus                                            &lt;chr&gt; NA, &quot;not_detect…
## $ Parainfluenza.4                                       &lt;chr&gt; NA, &quot;not_detect…
## $ Coronavirus229E                                       &lt;chr&gt; NA, &quot;not_detect…
## $ CoronavirusOC43                                       &lt;chr&gt; NA, &quot;not_detect…
## $ Inf.A.H1N1.2009                                       &lt;chr&gt; NA, &quot;not_detect…
## $ Bordetella.pertussis                                  &lt;chr&gt; NA, &quot;not_detect…
## $ Metapneumovirus                                       &lt;chr&gt; NA, &quot;not_detect…
## $ Parainfluenza.2                                       &lt;chr&gt; NA, &quot;not_detect…
## $ Neutrophils                                           &lt;dbl&gt; NA, -0.6190860,…
## $ Urea                                                  &lt;dbl&gt; NA, 1.19805908,…
## $ Proteina.C.reativa.mg.dL                              &lt;dbl&gt; NA, -0.1478949,…
## $ Creatinine                                            &lt;dbl&gt; NA, 2.0899284, …
## $ Potassium                                             &lt;dbl&gt; NA, -0.3057871,…
## $ Sodium                                                &lt;dbl&gt; NA, 0.8625116, …
## $ Influenza.B..rapid.test                               &lt;chr&gt; NA, &quot;negative&quot;,…
## $ Influenza.A..rapid.test                               &lt;chr&gt; NA, &quot;negative&quot;,…
## $ Alanine.transaminase                                  &lt;dbl&gt; NA, NA, NA, NA,…
## $ Aspartate.transaminase                                &lt;dbl&gt; NA, NA, NA, NA,…
## $ Gamma.glutamyltransferase.                            &lt;dbl&gt; NA, NA, NA, NA,…
## $ Total.Bilirubin                                       &lt;dbl&gt; NA, NA, NA, NA,…
## $ Direct.Bilirubin                                      &lt;dbl&gt; NA, NA, NA, NA,…
## $ Indirect.Bilirubin                                    &lt;dbl&gt; NA, NA, NA, NA,…
## $ Alkaline.phosphatase                                  &lt;dbl&gt; NA, NA, NA, NA,…
## $ Ionized.calcium.                                      &lt;dbl&gt; NA, NA, NA, NA,…
## $ Strepto.A                                             &lt;chr&gt; NA, NA, NA, NA,…
## $ Magnesium                                             &lt;dbl&gt; NA, NA, NA, NA,…
## $ pCO2..venous.blood.gas.analysis.                      &lt;dbl&gt; NA, NA, NA, NA,…
## $ Hb.saturation..venous.blood.gas.analysis.             &lt;dbl&gt; NA, NA, NA, NA,…
## $ Base.excess..venous.blood.gas.analysis.               &lt;dbl&gt; NA, NA, NA, NA,…
## $ pO2..venous.blood.gas.analysis.                       &lt;dbl&gt; NA, NA, NA, NA,…
## $ Fio2..venous.blood.gas.analysis.                      &lt;lgl&gt; NA, NA, NA, NA,…
## $ Total.CO2..venous.blood.gas.analysis.                 &lt;dbl&gt; NA, NA, NA, NA,…
## $ pH..venous.blood.gas.analysis.                        &lt;dbl&gt; NA, NA, NA, NA,…
## $ HCO3..venous.blood.gas.analysis.                      &lt;dbl&gt; NA, NA, NA, NA,…
## $ Rods..                                                &lt;dbl&gt; NA, NA, NA, NA,…
## $ Segmented                                             &lt;dbl&gt; NA, NA, NA, NA,…
## $ Promyelocytes                                         &lt;dbl&gt; NA, NA, NA, NA,…
## $ Metamyelocytes                                        &lt;dbl&gt; NA, NA, NA, NA,…
## $ Myelocytes                                            &lt;dbl&gt; NA, NA, NA, NA,…
## $ Myeloblasts                                           &lt;dbl&gt; NA, NA, NA, NA,…
## $ Urine...Esterase                                      &lt;chr&gt; NA, NA, NA, NA,…
## $ Urine...Aspect                                        &lt;chr&gt; NA, NA, NA, NA,…
## $ Urine...pH                                            &lt;chr&gt; NA, NA, NA, NA,…
## $ Urine...Hemoglobin                                    &lt;chr&gt; NA, NA, NA, NA,…
## $ Urine...Bile.pigments                                 &lt;chr&gt; NA, NA, NA, NA,…
## $ Urine...Ketone.Bodies                                 &lt;chr&gt; NA, NA, NA, NA,…
## $ Urine...Nitrite                                       &lt;chr&gt; NA, NA, NA, NA,…
## $ Urine...Density                                       &lt;dbl&gt; NA, NA, NA, NA,…
## $ Urine...Urobilinogen                                  &lt;chr&gt; NA, NA, NA, NA,…
## $ Urine...Protein                                       &lt;chr&gt; NA, NA, NA, NA,…
## $ Urine...Sugar                                         &lt;lgl&gt; NA, NA, NA, NA,…
## $ Urine...Leukocytes                                    &lt;chr&gt; NA, NA, NA, NA,…
## $ Urine...Crystals                                      &lt;chr&gt; NA, NA, NA, NA,…
## $ Urine...Red.blood.cells                               &lt;dbl&gt; NA, NA, NA, NA,…
## $ Urine...Hyaline.cylinders                             &lt;chr&gt; NA, NA, NA, NA,…
## $ Urine...Granular.cylinders                            &lt;chr&gt; NA, NA, NA, NA,…
## $ Urine...Yeasts                                        &lt;chr&gt; NA, NA, NA, NA,…
## $ Urine...Color                                         &lt;chr&gt; NA, NA, NA, NA,…
## $ Partial.thromboplastin.time..PTT..                    &lt;lgl&gt; NA, NA, NA, NA,…
## $ Relationship..Patient.Normal.                         &lt;dbl&gt; NA, NA, NA, NA,…
## $ International.normalized.ratio..INR.                  &lt;dbl&gt; NA, NA, NA, NA,…
## $ Lactic.Dehydrogenase                                  &lt;dbl&gt; NA, NA, NA, NA,…
## $ Prothrombin.time..PT...Activity                       &lt;lgl&gt; NA, NA, NA, NA,…
## $ Vitamin.B12                                           &lt;dbl&gt; NA, NA, NA, NA,…
## $ Creatine.phosphokinase..CPK..                         &lt;dbl&gt; NA, NA, NA, NA,…
## $ Ferritin                                              &lt;dbl&gt; NA, NA, NA, NA,…
## $ Arterial.Lactic.Acid                                  &lt;dbl&gt; NA, NA, NA, NA,…
## $ Lipase.dosage                                         &lt;lgl&gt; NA, NA, NA, NA,…
## $ D.Dimer                                               &lt;lgl&gt; NA, NA, NA, NA,…
## $ Albumin                                               &lt;dbl&gt; NA, NA, NA, NA,…
## $ Hb.saturation..arterial.blood.gases.                  &lt;dbl&gt; NA, NA, NA, NA,…
## $ pCO2..arterial.blood.gas.analysis.                    &lt;dbl&gt; NA, NA, NA, NA,…
## $ Base.excess..arterial.blood.gas.analysis.             &lt;dbl&gt; NA, NA, NA, NA,…
## $ pH..arterial.blood.gas.analysis.                      &lt;dbl&gt; NA, NA, NA, NA,…
## $ Total.CO2..arterial.blood.gas.analysis.               &lt;dbl&gt; NA, NA, NA, NA,…
## $ HCO3..arterial.blood.gas.analysis.                    &lt;dbl&gt; NA, NA, NA, NA,…
## $ pO2..arterial.blood.gas.analysis.                     &lt;dbl&gt; NA, NA, NA, NA,…
## $ Arteiral.Fio2                                         &lt;dbl&gt; NA, NA, NA, NA,…
## $ Phosphor                                              &lt;dbl&gt; NA, NA, NA, NA,…
## $ ctO2..arterial.blood.gas.analysis.                    &lt;dbl&gt; NA, NA, NA, NA,…</code></pre>
<pre class="r"><code># remove columns that won&#39;t help on diagnosis

dataset_clean &lt;- dataset %&gt;%
  select(-Patient.ID, 
         -Patient.addmited.to.regular.ward..1.yes..0.no.,
         -Patient.addmited.to.semi.intensive.unit..1.yes..0.no.,
         -Patient.addmited.to.intensive.care.unit..1.yes..0.no.)

# convert level Urine...Leukocytes &lt;1000 to 1000

dataset_clean$Urine...Leukocytes[dataset_clean$`Urine...Leukocytes` == &quot;&lt;1000&quot;] &lt;- 1000
dataset_clean$`Urine...Leukocytes` &lt;- as.numeric(dataset_clean$`Urine...Leukocytes`)

# fix Urine...pH

dataset_clean$`Urine...pH`[dataset_clean$`Urine...pH` == &quot;Não Realizado&quot;] &lt;- NA
dataset_clean$`Urine...pH` &lt;- as.numeric(dataset_clean$`Urine...pH`)

# Urine...Hemoglobin

dataset_clean$`Urine...Hemoglobin`[dataset_clean$`Urine...Hemoglobin` == &quot;not_done&quot;] &lt;- NA

# Urine...Aspect

dataset_clean$`Urine...Aspect` &lt;- factor(dataset_clean$`Urine...Aspect`, 
                                         levels = c(&quot;clear&quot;, &quot;lightly_cloudy&quot;, &quot;cloudy&quot;, &quot;altered_coloring&quot;))

# Strepto A

dataset_clean$`Strepto.A`[dataset_clean$`Strepto.A` == &quot;not_done&quot;] &lt;- NA

# transform character to factor

dataset_clean_num &lt;- dataset_clean %&gt;%
  select_if(is.numeric)

dataset_clean_cat &lt;- dataset_clean %&gt;%
  select_if(negate(is.numeric)) %&gt;%
  mutate_all(as.factor)

dataset_clean &lt;- base::cbind(dataset_clean_num, dataset_clean_cat)

# fix factor levels

# sort(sapply(dataset_clean[,sapply(dataset_clean, is.factor)], nlevels))</code></pre>
<p>Com os dados lidos e processados, eu dou uma olhada nos dados faltantes:</p>
<pre class="r"><code># let&#39;s take a look on missing data

missing_values &lt;- dataset_clean %&gt;%
  gather(key = &quot;key&quot;, value = &quot;val&quot;) %&gt;%
  mutate(is.missing = is.na(val)) %&gt;%
  group_by(key, is.missing) %&gt;%
  summarise(num.missing = n()) %&gt;%
  filter(is.missing == TRUE) %&gt;%
  select(-is.missing) %&gt;%
  ungroup() %&gt;%
  mutate(key = reorder(key, -num.missing)) %&gt;%
  arrange(desc(num.missing)) %&gt;%
  print(n = Inf)</code></pre>
<pre><code>## Warning: attributes are not identical across measure variables;
## they will be dropped</code></pre>
<pre><code>## # A tibble: 105 x 2
##     key                                              num.missing
##     &lt;fct&gt;                                                  &lt;int&gt;
##   1 D.Dimer                                                 5644
##   2 Mycoplasma.pneumoniae                                   5644
##   3 Partial.thromboplastin.time..PTT..                      5644
##   4 Prothrombin.time..PT...Activity                         5644
##   5 Urine...Sugar                                           5644
##   6 Fio2..venous.blood.gas.analysis.                        5643
##   7 Urine...Nitrite                                         5643
##   8 Vitamin.B12                                             5641
##   9 Lipase.dosage                                           5636
##  10 Albumin                                                 5631
##  11 Arteiral.Fio2                                           5624
##  12 Phosphor                                                5624
##  13 Ferritin                                                5621
##  14 Arterial.Lactic.Acid                                    5617
##  15 Base.excess..arterial.blood.gas.analysis.               5617
##  16 ctO2..arterial.blood.gas.analysis.                      5617
##  17 Hb.saturation..arterial.blood.gases.                    5617
##  18 HCO3..arterial.blood.gas.analysis.                      5617
##  19 pCO2..arterial.blood.gas.analysis.                      5617
##  20 pH..arterial.blood.gas.analysis.                        5617
##  21 pO2..arterial.blood.gas.analysis.                       5617
##  22 Total.CO2..arterial.blood.gas.analysis.                 5617
##  23 Magnesium                                               5604
##  24 Ionized.calcium.                                        5594
##  25 Urine...Ketone.Bodies                                   5587
##  26 Urine...Esterase                                        5584
##  27 Urine...Protein                                         5584
##  28 Urine...Hyaline.cylinders                               5577
##  29 Urine...Granular.cylinders                              5575
##  30 Urine...Hemoglobin                                      5575
##  31 Urine...pH                                              5575
##  32 Urine...Urobilinogen                                    5575
##  33 Urine...Aspect                                          5574
##  34 Urine...Bile.pigments                                   5574
##  35 Urine...Color                                           5574
##  36 Urine...Crystals                                        5574
##  37 Urine...Density                                         5574
##  38 Urine...Leukocytes                                      5574
##  39 Urine...Red.blood.cells                                 5574
##  40 Urine...Yeasts                                          5574
##  41 Relationship..Patient.Normal.                           5553
##  42 Metamyelocytes                                          5547
##  43 Myeloblasts                                             5547
##  44 Myelocytes                                              5547
##  45 Promyelocytes                                           5547
##  46 Rods..                                                  5547
##  47 Segmented                                               5547
##  48 Lactic.Dehydrogenase                                    5543
##  49 Creatine.phosphokinase..CPK..                           5540
##  50 International.normalized.ratio..INR.                    5511
##  51 Base.excess..venous.blood.gas.analysis.                 5508
##  52 Hb.saturation..venous.blood.gas.analysis.               5508
##  53 HCO3..venous.blood.gas.analysis.                        5508
##  54 pCO2..venous.blood.gas.analysis.                        5508
##  55 pH..venous.blood.gas.analysis.                          5508
##  56 pO2..venous.blood.gas.analysis.                         5508
##  57 Total.CO2..venous.blood.gas.analysis.                   5508
##  58 Alkaline.phosphatase                                    5500
##  59 Gamma.glutamyltransferase.                              5491
##  60 Direct.Bilirubin                                        5462
##  61 Indirect.Bilirubin                                      5462
##  62 Total.Bilirubin                                         5462
##  63 Serum.Glucose                                           5436
##  64 Alanine.transaminase                                    5419
##  65 Aspartate.transaminase                                  5418
##  66 Strepto.A                                               5313
##  67 Sodium                                                  5274
##  68 Potassium                                               5273
##  69 Urea                                                    5247
##  70 Creatinine                                              5220
##  71 Proteina.C.reativa.mg.dL                                5138
##  72 Neutrophils                                             5131
##  73 Mean.platelet.volume                                    5045
##  74 Monocytes                                               5043
##  75 Basophils                                               5042
##  76 Eosinophils                                             5042
##  77 Leukocytes                                              5042
##  78 Lymphocytes                                             5042
##  79 Mean.corpuscular.hemoglobin..MCH.                       5042
##  80 Mean.corpuscular.hemoglobin.concentration..MCHC.        5042
##  81 Mean.corpuscular.volume..MCV.                           5042
##  82 Platelets                                               5042
##  83 Red.blood.cell.distribution.width..RDW.                 5042
##  84 Red.blood.Cells                                         5042
##  85 Hematocrit                                              5041
##  86 Hemoglobin                                              5041
##  87 Influenza.A..rapid.test                                 4824
##  88 Influenza.B..rapid.test                                 4824
##  89 Adenovirus                                              4292
##  90 Bordetella.pertussis                                    4292
##  91 Chlamydophila.pneumoniae                                4292
##  92 Coronavirus.HKU1                                        4292
##  93 Coronavirus229E                                         4292
##  94 CoronavirusNL63                                         4292
##  95 CoronavirusOC43                                         4292
##  96 Inf.A.H1N1.2009                                         4292
##  97 Metapneumovirus                                         4292
##  98 Parainfluenza.1                                         4292
##  99 Parainfluenza.2                                         4292
## 100 Parainfluenza.3                                         4292
## 101 Parainfluenza.4                                         4292
## 102 Rhinovirus.Enterovirus                                  4292
## 103 Influenza.A                                             4290
## 104 Influenza.B                                             4290
## 105 Respiratory.Syncytial.Virus                             4290</code></pre>
<pre class="r"><code>missing_values %&gt;%
  ggplot() +
  geom_bar(aes(x = key, y = 100*num.missing/dim(dataset_clean)[1]), stat = &quot;identity&quot;) +
  labs(x = &quot;Variable&quot;, y=&quot;Percent of missing values&quot;) +  
  theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 8))</code></pre>
<p><img src="/posts/covid-19-einstein-90-acuracia_files/figure-html/missingData01-1.png" width="672" /></p>
<p>Este gráfico mostra quais são as variáveis com maior quantidade de dados faltantes. Note que algumas delas possuem quase 100% de dados faltantes! Veja o gráfico a seguir para que tenhamos uma ideia melhor da magnitude da falta de dados que estamos enfrentando:</p>
<pre class="r"><code>vis_miss(dataset_clean) +
  theme(axis.text.x = element_text(size = 6))</code></pre>
<p><img src="/posts/covid-19-einstein-90-acuracia_files/figure-html/missingData02-1.png" width="672" /></p>
<p>Mais de 91% de dados faltantes é muita coisa. Este tipo de informação não me dá muita esperança em encontrar um modelo bom para estes dados. Mesmo assim, vou tentar algo e ver o que acontece. Para isso, vou manter no conjunto a ser analisado apenas as colunas com pelo menos 1000 observações. Ou seja, vou retirar todas as colunas com muito poucos dados registrados.</p>
<pre class="r"><code># keep only columns with at least n observations

n &lt;- 1000

dataset_clean &lt;- dataset_clean[, which(dim(dataset_clean)[1] - apply(apply(dataset_clean, 2, is.na), 2, sum) &gt;= n)]

# remove quantitative variables with variance equal to zero 

dataset_model_num &lt;- dataset_clean %&gt;%
  select_if(is.numeric)

if (sum(apply(dataset_model_num, 2, var, na.rm = TRUE) == 0) != 0) {
  dataset_model_num &lt;- dataset_model_num[, which(apply(dataset_model_num, 2, var, na.rm = TRUE) == 0)]
}

# remove categorical variables with only one level 

dataset_model_cat &lt;- dataset_clean %&gt;%
  select_if(negate(is.numeric))

dataset_model_cat &lt;- dataset_model_cat[, sapply(dataset_model_cat, nlevels) &gt; 1]
  
# final dataset

dataset_model &lt;- base::cbind(dataset_model_num, dataset_model_cat)

vis_miss(dataset_model) + # it needs naniar package
  theme(axis.text.x = element_text(size = 6))</code></pre>
<p><img src="/posts/covid-19-einstein-90-acuracia_files/figure-html/preparation-1.png" width="672" /></p>
<p>É possíver ver que há uma proporção menor de dados faltantes neste conjunto processado. Baixamos a proporção de dados faltantes de 91,4% para 68%. Ainda é um valor bastante alto, mas está um pouco menos pior do que o original.</p>
<p>Abaixo coloco mais alguns gráficos que tentam relacionar as variáveis que restaram no conjunto de dados, mas aparentemente não há relação alguma entre elas.</p>
<pre class="r"><code># some other plots

ggpairs(dataset_model[, c(1:10)])</code></pre>
<pre><code>## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.</code></pre>
<p><img src="/posts/covid-19-einstein-90-acuracia_files/figure-html/edaSimples-1.png" width="672" /></p>
<pre class="r"><code>ggpairs(dataset_model[, c(11:ncol(dataset_model))])</code></pre>
<p><img src="/posts/covid-19-einstein-90-acuracia_files/figure-html/edaSimples-2.png" width="672" /></p>
<pre class="r"><code># it seems there is no relation between any pair of variables</code></pre>
</div>
<div id="modelagem-1" class="section level1">
<h1>Modelagem 1</h1>
<p>Poucos modelos de aprendizagem de máquina conseguem lidar com dados faltantes. Por este motivo, decidi escolher um modelo chamado <a href="https://en.wikipedia.org/wiki/Decision_tree_learning">CART</a> (Classification and Regression Tree - Árvore de Classificação e Regressão). Abaixo estão os resultados obtidos, após a separação do conjunto de dados original em treinamento e teste. Os resultados reportados foram obtidos após a aplicação do modelo ajustado no conjunto de teste. Para saber mais porque fazer isso, recomendo novamente meu post <a href="https://marcusnunes.me/posts/primeiro-projeto-de-data-science/">Tutorial: Como Fazer o Seu Primeiro Projeto de Data Science</a>.</p>
<pre class="r"><code>################
### modeling ###
################

# train/test split

covid &lt;- dataset_model

set.seed(1)

index       &lt;- createDataPartition(covid$SARS.Cov.2.exam.result, 
                                   p = 0.75, 
                                   list = FALSE)
covid_train &lt;- covid[ index, ]
covid_test  &lt;- covid[-index, ]

dim(covid_train)
table(covid_train$SARS.Cov.2.exam.result)

dim(covid_test)
table(covid_test$SARS.Cov.2.exam.result)

# parameters for cart

fitControl &lt;- trainControl(method = &quot;cv&quot;,
                           number = 5,
                           savePred = TRUE, 
                           classProb = TRUE)

tune.grid &lt;- expand.grid(mincriterion = seq(from = 0.01, 
                                            to = .99, 
                                            by = 0.01))

set.seed(1)

x &lt;- covid_train %&gt;%
  select(-SARS.Cov.2.exam.result)

y &lt;- covid_train %&gt;%
  select(SARS.Cov.2.exam.result) %&gt;%
  unlist()

covid_ctree &lt;- train(x, y,
                     method = &quot;ctree&quot;, 
                     tuneGrid = tune.grid,
                     trControl = fitControl)</code></pre>
<pre class="r"><code>ggplot(covid_ctree)</code></pre>
<p><img src="/posts/covid-19-einstein-90-acuracia_files/figure-html/cart02-1.png" width="672" /></p>
<pre class="r"><code>prediction &lt;- predict(covid_ctree, covid_test)

confusionMatrix(prediction, covid_test$SARS.Cov.2.exam.result)</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction negative positive
##   negative     1271      139
##   positive        0        0
##                                           
##                Accuracy : 0.9014          
##                  95% CI : (0.8847, 0.9165)
##     No Information Rate : 0.9014          
##     P-Value [Acc &gt; NIR] : 0.5226          
##                                           
##                   Kappa : 0               
##                                           
##  Mcnemar&#39;s Test P-Value : &lt;2e-16          
##                                           
##             Sensitivity : 1.0000          
##             Specificity : 0.0000          
##          Pos Pred Value : 0.9014          
##          Neg Pred Value :    NaN          
##              Prevalence : 0.9014          
##          Detection Rate : 0.9014          
##    Detection Prevalence : 1.0000          
##       Balanced Accuracy : 0.5000          
##                                           
##        &#39;Positive&#39; Class : negative        
## </code></pre>
<pre class="r"><code># 90% accuracy seems good, but No Information Rate is also 90%
# so, using this model or a random process gives the same answer
# high sensitivity, but very low specificity :(</code></pre>
<p>Note que obtive 90% de acurácia no meu modelo. Ou seja, ele acerta 90% das tentativas de classificar um paciente como positivo ou negativo em relação ao coronavírus. Parece um resultado muito bom, mas não é. Ele é péssimo, na realidade.</p>
<p>No conjunto de dados original, aproximadamente 90% dos pacientes não possuem coronavírus, enquanto 10% estão infectados. O meu modelo encontra, com 100% de certeza, quem não tem coronavírus (é o valor de <code>Sensitivity</code> no output acima). Entretanto, encontra 0% dos pacientes com coronavírus (é o valor de <code>Specificity</code> no output acima). Ou seja, ele vai mandar para casa todo mundo que chegar no hospital, tendo coronavírus ou não.</p>
<p>Isso me fez partir para uma segunda modelagem.</p>
</div>
<div id="modelagem-2" class="section level1">
<h1>Modelagem 2</h1>
<p>Vimos que a primeira modelagem não deu bons resultados. Com o intuito de tentar obter resultados melhores, vou proceder com a imputação de dados. Se for tentar rodar o código abaixo em seu computador, prepare-se para esperar uns bons minutos.</p>
<pre class="r"><code>#######################
### data imputation ###
#######################

covid_imp &lt;- mice(covid, meth = &quot;rf&quot;, ntree = 5) # be patient

covid &lt;- complete(covid_imp)</code></pre>
<pre class="r"><code>################
### modeling ###
################

# train/test split

set.seed(1)

index       &lt;- createDataPartition(covid$SARS.Cov.2.exam.result, 
                                   p = 0.75, 
                                   list = FALSE)
covid_train &lt;- covid[ index, ]
covid_test  &lt;- covid[-index, ]

dim(covid_train)
table(covid_train$SARS.Cov.2.exam.result)

dim(covid_test)
table(covid_test$SARS.Cov.2.exam.result)

# parameters for random forest

fitControl &lt;- trainControl(method = &quot;cv&quot;,
                           number = 5,
                           savePred = TRUE, 
                           classProb = TRUE)

tune.grid &lt;- expand.grid(mtry = 1:35)

set.seed(1)

x &lt;- covid_train %&gt;%
  select(-SARS.Cov.2.exam.result)

y &lt;- covid_train %&gt;%
  select(SARS.Cov.2.exam.result) %&gt;%
  unlist()

covid_rf &lt;- train(x, y,
                  method = &quot;rf&quot;, 
                  tuneGrid = tune.grid,
                  trControl = fitControl)</code></pre>
<pre class="r"><code>ggplot(covid_rf)</code></pre>
<p><img src="/posts/covid-19-einstein-90-acuracia_files/figure-html/random_forest2-1.png" width="672" /></p>
<pre class="r"><code>prediction &lt;- predict(covid_rf, covid_test)

confusionMatrix(prediction, covid_test$SARS.Cov.2.exam.result)</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction negative positive
##   negative     1271      139
##   positive        0        0
##                                           
##                Accuracy : 0.9014          
##                  95% CI : (0.8847, 0.9165)
##     No Information Rate : 0.9014          
##     P-Value [Acc &gt; NIR] : 0.5226          
##                                           
##                   Kappa : 0               
##                                           
##  Mcnemar&#39;s Test P-Value : &lt;2e-16          
##                                           
##             Sensitivity : 1.0000          
##             Specificity : 0.0000          
##          Pos Pred Value : 0.9014          
##          Neg Pred Value :    NaN          
##              Prevalence : 0.9014          
##          Detection Rate : 0.9014          
##    Detection Prevalence : 1.0000          
##       Balanced Accuracy : 0.5000          
##                                           
##        &#39;Positive&#39; Class : negative        
## </code></pre>
<pre class="r"><code># high sensitivity, but very low specificity :(</code></pre>
<p>De novo, o mesmo problema: alta sensitividade e baixíssima especificidade neste modelo. Ou seja, não é melhor do que o modelo anterior ou uma seleção aleatória de diagnóstico.</p>
</div>
<div id="conclusão" class="section level1">
<h1>Conclusão</h1>
<p>Eu cheguei no meu modelo final. 90% de acurácia usando random forest. Mas como a variável resposta é desbalanceada, com 90% para uma classe e 10% para outra, este meu modelo não serve pra nada.</p>
<p>Mesmo tentando abordagens diversas, como filtragem de dados faltantes e imputação, nada deu certo pra mim. Aparentemente este é um resultado geral para este problema. Os outros participantes deste desafio no kaggle criaram várias <a href="https://www.kaggle.com/einsteindata4u/covid19/kernels">análises diferentes</a>, com muitas abordagens interessantes, mas ninguém consegue uma boa detecção de verdadeiros positivos.</p>
<p>Tenho pouca esperança que, com este conjunto de dados específico, seja possível fazer algo preditivo de qualidade. Talvez com uma feature engineering muito boa? Pode ser, mas não podemos esquecer que são muitos dados faltantes. No conjunto original, sem pré-processamento, são 91% de dados faltantes. Não dá pra fazer milagre.</p>
<p>Como sempre, o código utilizado nesta análise está disponível em meu <a href="https://github.com/mnunes/einstein-covid-19">github</a>.</p>
</div>
]]></content>
		</item>
		
		<item>
			<title>Análise Descritiva do Coronavírus nos Estados Brasileiros</title>
			<link>https://marcusnunes.me/posts/analise-descritiva-do-coronavirus/</link>
			<pubDate>Tue, 24 Mar 2020 10:12:00 -0300</pubDate>
			
			<guid>https://marcusnunes.me/posts/analise-descritiva-do-coronavirus/</guid>
			<description>Justificativa Hoje, 24 de março de 2020, o coronavírus está muito presente em todas as conversas dos brasileiros. Aqueles que podem estão em quarentena, tentando evitar ao máximo que a doença se espalhe.
Estou aproveitando este tempo de molho para aprender sobre um assunto que domino muito pouco: a criação de mapas no R. Por isso, resolvi utilizar os dados disponibilizados pela equipe do site Brasil.</description>
			<content type="html"><![CDATA[
<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>
<script src="/rmarkdown-libs/htmlwidgets/htmlwidgets.js"></script>
<script src="/rmarkdown-libs/plotly-binding/plotly.js"></script>
<script src="/rmarkdown-libs/typedarray/typedarray.min.js"></script>
<script src="/rmarkdown-libs/jquery/jquery.min.js"></script>
<link href="/rmarkdown-libs/crosstalk/css/crosstalk.css" rel="stylesheet" />
<script src="/rmarkdown-libs/crosstalk/js/crosstalk.min.js"></script>
<link href="/rmarkdown-libs/plotly-htmlwidgets-css/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="/rmarkdown-libs/plotly-main/plotly-latest.min.js"></script>


<div id="justificativa" class="section level1">
<h1>Justificativa</h1>
<p>Hoje, 24 de março de 2020, o coronavírus está muito presente em todas as conversas dos brasileiros. Aqueles que podem estão em quarentena, tentando evitar ao máximo que a doença se espalhe.</p>
<p>Estou aproveitando este tempo de molho para aprender sobre um assunto que domino muito pouco: a criação de mapas no R. Por isso, resolvi utilizar os dados disponibilizados pela equipe do site <a href="https://brasil.io/dataset/covid19/caso">Brasil.io</a> e realizar uma análise espacial simples, a nível estadual, tentando identificar os estados brasileiros com a maior taxa de casos confirmados.</p>
<p>Optei por utilizar uma escala baseada na taxa de casos confirmados por 100.000 habitantes por 2 motivos principais:</p>
<ol style="list-style-type: decimal">
<li>Acredito que comparar valores absolutos entre estados não é a melhor maneira de entender a proliferação da doença, pois São Paulo possui muito mais habitantes do que Roraima, por exemplo;</li>
<li>Casos suspeitos poderiam deixar a análise ainda mais enviesada do que ela já provavelmente está.</li>
</ol>
<p>Com isso, quero dizer que sei das limitações da minha análise, especialmente no que tange à subnotificação de casos. Além de termos limitações me relação à verba que cada estado da federação possui em caixa para comprar os testes necessários para identificação da doença.</p>
<p>Além disso, nem todas as secretarias de saúde estaduais possuem a mesma velocidade de divulgação nos dados. Assim, apesar de estarmos em 24 de março, estou usando os dados do dia 21 deste mês. Qualquer um pode realizar novamente esta análise para outros dias alterando o valor da variável <code>hoje</code> colocada no código abaixo.</p>
</div>
<div id="preparação-dos-dados" class="section level1">
<h1>Preparação dos Dados</h1>
<p>Esta parte pode ser pulada por quem não está interessado em reproduzir este estudo. Basicamente o que faço abaixo é carregar pacotes necessários para a minha análise, ler os dados no R e prepará-los para aquilo que me interessa.</p>
<pre class="r"><code>library(tidyverse)</code></pre>
<pre><code>## ── Attaching packages ───────────────────────────────────────── tidyverse 1.3.0 ──</code></pre>
<pre><code>## ✓ ggplot2 3.3.0     ✓ purrr   0.3.3
## ✓ tibble  3.0.0     ✓ dplyr   0.8.5
## ✓ tidyr   1.0.2     ✓ stringr 1.4.0
## ✓ readr   1.3.1     ✓ forcats 0.5.0</code></pre>
<pre><code>## ── Conflicts ──────────────────────────────────────────── tidyverse_conflicts() ──
## x dplyr::filter() masks stats::filter()
## x dplyr::lag()    masks stats::lag()</code></pre>
<pre class="r"><code>theme_set(theme_bw())
library(lubridate)</code></pre>
<pre><code>## 
## Attaching package: &#39;lubridate&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:base&#39;:
## 
##     date</code></pre>
<pre class="r"><code>library(plotly)</code></pre>
<pre><code>## 
## Attaching package: &#39;plotly&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:ggplot2&#39;:
## 
##     last_plot</code></pre>
<pre><code>## The following object is masked from &#39;package:stats&#39;:
## 
##     filter</code></pre>
<pre><code>## The following object is masked from &#39;package:graphics&#39;:
## 
##     layout</code></pre>
<pre class="r"><code>library(brazilmaps)

############################
### preparacao dos dados ###
############################

# dados obtidos partir do site brasil.io:
# 
# https://brasil.io/dataset/covid19/caso

corona &lt;- read.csv(file=&quot;https://brasil.io/dataset/covid19/caso?format=csv&quot;)
corona &lt;- as_tibble(corona)

# transformar coluna `date` em data

corona &lt;- corona %&gt;% 
  mutate(date = ymd(date))

# transformar a coluna `is_last` em logical

corona &lt;- corona %&gt;% 
  mutate(is_last = ifelse(is_last == &quot;True&quot;, TRUE, FALSE))

# remover as linhas com totais por cidade

corona &lt;- corona %&gt;% 
  filter(place_type == &quot;state&quot;)</code></pre>
<p>Com isso os dados do site <a href="https://brasil.io/dataset/covid19/caso">Brasil.io</a> estão prontos para a análise que delineeei.</p>
</div>
<div id="análise-exploratória-dos-dados" class="section level1">
<h1>Análise Exploratória dos Dados</h1>
<p>A primeira análise que realizo é a comparação dos casos totais em cinco estados que considero importantes: SP, RJ e MG, por possuírem os três maiores PIBs do país, além de RS e RN, o estado em que nasci e o estado em que moro atualmente. Devido à velocidade de atualização dos dados estaduais, minha análise se refere à situação do dia 21 de março de 2020. Para realizar estudos mais atuais, substitua o valor da variável <code>hoje</code> pelo dia da sua preferência no formato aaaa-mm-dd.</p>
<pre class="r"><code>######################################
### analise exploratoria dos dados ###
######################################

# altere a data abaixo, no formato aaaa-mm-dd para realizar 
# esta análise para outro dia

hoje &lt;- ymd(&quot;2020-03-21&quot;)

corona %&gt;%
  filter(date == hoje) %&gt;%
  arrange(desc(confirmed)) %&gt;%
  select(state, confirmed) %&gt;%
  print(n = Inf)</code></pre>
<pre><code>## # A tibble: 26 x 2
##    state confirmed
##    &lt;fct&gt;     &lt;int&gt;
##  1 SP          396
##  2 RJ          119
##  3 DF          108
##  4 CE           84
##  5 RS           69
##  6 SC           57
##  7 MG           55
##  8 PR           43
##  9 BA           41
## 10 PE           33
## 11 ES           26
## 12 GO           19
## 13 MS           16
## 14 AC           11
## 15 AM           11
## 16 SE           10
## 17 RN            9
## 18 AL            7
## 19 PI            4
## 20 RO            3
## 21 MA            2
## 22 PA            2
## 23 RR            2
## 24 TO            2
## 25 AP            1
## 26 PB            1</code></pre>
<pre class="r"><code>g &lt;- corona %&gt;%
  filter(state %in% c(&quot;SP&quot;, &quot;RJ&quot;, &quot;MG&quot;, &quot;RS&quot;, &quot;RN&quot;)) %&gt;%
  group_by(date) %&gt;%
  #top_n(1, confirmed) %&gt;%
  ggplot(., aes(x = date, y = confirmed, group = state, colour = state)) +
  geom_line() +
  labs(x = &quot;Data&quot;, y = &quot;Casos Confirmados&quot;, colour = &quot;Estado&quot;) +
  scale_colour_viridis_d()

ggplotly(g)</code></pre>
<div id="htmlwidget-1" style="width:672px;height:480px;" class="plotly html-widget"></div>
<script type="application/json" data-for="htmlwidget-1">{"x":{"data":[{"x":[18329,18330,18331,18332,18333,18334,18335,18337,18338,18339,18340,18341,18342,18343,18344,18345,18346,18347,18348,18349,18350,18351,18352,18353],"y":[1,1,1,1,2,2,4,6,14,19,29,38,55,83,128,130,133,153,189,205,231,261,275,314],"text":["date: 2020-03-08<br />confirmed:    1<br />state: MG<br />state: MG","date: 2020-03-09<br />confirmed:    1<br />state: MG<br />state: MG","date: 2020-03-10<br />confirmed:    1<br />state: MG<br />state: MG","date: 2020-03-11<br />confirmed:    1<br />state: MG<br />state: MG","date: 2020-03-12<br />confirmed:    2<br />state: MG<br />state: MG","date: 2020-03-13<br />confirmed:    2<br />state: MG<br />state: MG","date: 2020-03-14<br />confirmed:    4<br />state: MG<br />state: MG","date: 2020-03-16<br />confirmed:    6<br />state: MG<br />state: MG","date: 2020-03-17<br />confirmed:   14<br />state: MG<br />state: MG","date: 2020-03-18<br />confirmed:   19<br />state: MG<br />state: MG","date: 2020-03-19<br />confirmed:   29<br />state: MG<br />state: MG","date: 2020-03-20<br />confirmed:   38<br />state: MG<br />state: MG","date: 2020-03-21<br />confirmed:   55<br />state: MG<br />state: MG","date: 2020-03-22<br />confirmed:   83<br />state: MG<br />state: MG","date: 2020-03-23<br />confirmed:  128<br />state: MG<br />state: MG","date: 2020-03-24<br />confirmed:  130<br />state: MG<br />state: MG","date: 2020-03-25<br />confirmed:  133<br />state: MG<br />state: MG","date: 2020-03-26<br />confirmed:  153<br />state: MG<br />state: MG","date: 2020-03-27<br />confirmed:  189<br />state: MG<br />state: MG","date: 2020-03-28<br />confirmed:  205<br />state: MG<br />state: MG","date: 2020-03-29<br />confirmed:  231<br />state: MG<br />state: MG","date: 2020-03-30<br />confirmed:  261<br />state: MG<br />state: MG","date: 2020-03-31<br />confirmed:  275<br />state: MG<br />state: MG","date: 2020-04-01<br />confirmed:  314<br />state: MG<br />state: MG"],"type":"scatter","mode":"lines","line":{"width":1.88976377952756,"color":"rgba(68,1,84,1)","dash":"solid"},"hoveron":"points","name":"MG","legendgroup":"MG","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null},{"x":[18326,18327,18329,18333,18336,18337,18339,18340,18342,18343,18344,18345,18346,18347,18348,18349,18350,18351,18352,18353],"y":[1,2,3,15,24,25,63,66,119,186,233,305,370,421,493,558,600,657,708,832],"text":["date: 2020-03-05<br />confirmed:    1<br />state: RJ<br />state: RJ","date: 2020-03-06<br />confirmed:    2<br />state: RJ<br />state: RJ","date: 2020-03-08<br />confirmed:    3<br />state: RJ<br />state: RJ","date: 2020-03-12<br />confirmed:   15<br />state: RJ<br />state: RJ","date: 2020-03-15<br />confirmed:   24<br />state: RJ<br />state: RJ","date: 2020-03-16<br />confirmed:   25<br />state: RJ<br />state: RJ","date: 2020-03-18<br />confirmed:   63<br />state: RJ<br />state: RJ","date: 2020-03-19<br />confirmed:   66<br />state: RJ<br />state: RJ","date: 2020-03-21<br />confirmed:  119<br />state: RJ<br />state: RJ","date: 2020-03-22<br />confirmed:  186<br />state: RJ<br />state: RJ","date: 2020-03-23<br />confirmed:  233<br />state: RJ<br />state: RJ","date: 2020-03-24<br />confirmed:  305<br />state: RJ<br />state: RJ","date: 2020-03-25<br />confirmed:  370<br />state: RJ<br />state: RJ","date: 2020-03-26<br />confirmed:  421<br />state: RJ<br />state: RJ","date: 2020-03-27<br />confirmed:  493<br />state: RJ<br />state: RJ","date: 2020-03-28<br />confirmed:  558<br />state: RJ<br />state: RJ","date: 2020-03-29<br />confirmed:  600<br />state: RJ<br />state: RJ","date: 2020-03-30<br />confirmed:  657<br />state: RJ<br />state: RJ","date: 2020-03-31<br />confirmed:  708<br />state: RJ<br />state: RJ","date: 2020-04-01<br />confirmed:  832<br />state: RJ<br />state: RJ"],"type":"scatter","mode":"lines","line":{"width":1.88976377952756,"color":"rgba(59,82,139,1)","dash":"solid"},"hoveron":"points","name":"RJ","legendgroup":"RJ","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null},{"x":[18333,18334,18337,18339,18341,18342,18343,18345,18346,18347,18348,18349,18351,18352,18353],"y":[1,1,1,1,6,9,13,14,14,19,28,68,77,82,92],"text":["date: 2020-03-12<br />confirmed:    1<br />state: RN<br />state: RN","date: 2020-03-13<br />confirmed:    1<br />state: RN<br />state: RN","date: 2020-03-16<br />confirmed:    1<br />state: RN<br />state: RN","date: 2020-03-18<br />confirmed:    1<br />state: RN<br />state: RN","date: 2020-03-20<br />confirmed:    6<br />state: RN<br />state: RN","date: 2020-03-21<br />confirmed:    9<br />state: RN<br />state: RN","date: 2020-03-22<br />confirmed:   13<br />state: RN<br />state: RN","date: 2020-03-24<br />confirmed:   14<br />state: RN<br />state: RN","date: 2020-03-25<br />confirmed:   14<br />state: RN<br />state: RN","date: 2020-03-26<br />confirmed:   19<br />state: RN<br />state: RN","date: 2020-03-27<br />confirmed:   28<br />state: RN<br />state: RN","date: 2020-03-28<br />confirmed:   68<br />state: RN<br />state: RN","date: 2020-03-30<br />confirmed:   77<br />state: RN<br />state: RN","date: 2020-03-31<br />confirmed:   82<br />state: RN<br />state: RN","date: 2020-04-01<br />confirmed:   92<br />state: RN<br />state: RN"],"type":"scatter","mode":"lines","line":{"width":1.88976377952756,"color":"rgba(33,144,140,1)","dash":"solid"},"hoveron":"points","name":"RN","legendgroup":"RN","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null},{"x":[18331,18332,18333,18334,18337,18339,18340,18341,18342,18343,18344,18345,18346,18347,18348,18349,18350,18351,18352,18353],"y":[1,2,4,4,8,19,31,43,69,77,96,112,162,190,197,200,230,254,305,316],"text":["date: 2020-03-10<br />confirmed:    1<br />state: RS<br />state: RS","date: 2020-03-11<br />confirmed:    2<br />state: RS<br />state: RS","date: 2020-03-12<br />confirmed:    4<br />state: RS<br />state: RS","date: 2020-03-13<br />confirmed:    4<br />state: RS<br />state: RS","date: 2020-03-16<br />confirmed:    8<br />state: RS<br />state: RS","date: 2020-03-18<br />confirmed:   19<br />state: RS<br />state: RS","date: 2020-03-19<br />confirmed:   31<br />state: RS<br />state: RS","date: 2020-03-20<br />confirmed:   43<br />state: RS<br />state: RS","date: 2020-03-21<br />confirmed:   69<br />state: RS<br />state: RS","date: 2020-03-22<br />confirmed:   77<br />state: RS<br />state: RS","date: 2020-03-23<br />confirmed:   96<br />state: RS<br />state: RS","date: 2020-03-24<br />confirmed:  112<br />state: RS<br />state: RS","date: 2020-03-25<br />confirmed:  162<br />state: RS<br />state: RS","date: 2020-03-26<br />confirmed:  190<br />state: RS<br />state: RS","date: 2020-03-27<br />confirmed:  197<br />state: RS<br />state: RS","date: 2020-03-28<br />confirmed:  200<br />state: RS<br />state: RS","date: 2020-03-29<br />confirmed:  230<br />state: RS<br />state: RS","date: 2020-03-30<br />confirmed:  254<br />state: RS<br />state: RS","date: 2020-03-31<br />confirmed:  305<br />state: RS<br />state: RS","date: 2020-04-01<br />confirmed:  316<br />state: RS<br />state: RS"],"type":"scatter","mode":"lines","line":{"width":1.88976377952756,"color":"rgba(93,200,99,1)","dash":"solid"},"hoveron":"points","name":"RS","legendgroup":"RS","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null},{"x":[18318,18319,18320,18321,18322,18323,18324,18325,18326,18327,18328,18329,18330,18331,18332,18333,18335,18337,18338,18339,18340,18341,18342,18343,18344,18345,18346,18347,18348,18349,18350,18351,18352,18353],"y":[1,1,2,2,2,2,2,3,6,10,13,16,16,19,30,46,65,152,164,240,286,396,396,631,745,810,862,1052,1223,1406,1451,1517,2339,2981],"text":["date: 2020-02-26<br />confirmed:    1<br />state: SP<br />state: SP","date: 2020-02-27<br />confirmed:    1<br />state: SP<br />state: SP","date: 2020-02-28<br />confirmed:    2<br />state: SP<br />state: SP","date: 2020-02-29<br />confirmed:    2<br />state: SP<br />state: SP","date: 2020-03-01<br />confirmed:    2<br />state: SP<br />state: SP","date: 2020-03-02<br />confirmed:    2<br />state: SP<br />state: SP","date: 2020-03-03<br />confirmed:    2<br />state: SP<br />state: SP","date: 2020-03-04<br />confirmed:    3<br />state: SP<br />state: SP","date: 2020-03-05<br />confirmed:    6<br />state: SP<br />state: SP","date: 2020-03-06<br />confirmed:   10<br />state: SP<br />state: SP","date: 2020-03-07<br />confirmed:   13<br />state: SP<br />state: SP","date: 2020-03-08<br />confirmed:   16<br />state: SP<br />state: SP","date: 2020-03-09<br />confirmed:   16<br />state: SP<br />state: SP","date: 2020-03-10<br />confirmed:   19<br />state: SP<br />state: SP","date: 2020-03-11<br />confirmed:   30<br />state: SP<br />state: SP","date: 2020-03-12<br />confirmed:   46<br />state: SP<br />state: SP","date: 2020-03-14<br />confirmed:   65<br />state: SP<br />state: SP","date: 2020-03-16<br />confirmed:  152<br />state: SP<br />state: SP","date: 2020-03-17<br />confirmed:  164<br />state: SP<br />state: SP","date: 2020-03-18<br />confirmed:  240<br />state: SP<br />state: SP","date: 2020-03-19<br />confirmed:  286<br />state: SP<br />state: SP","date: 2020-03-20<br />confirmed:  396<br />state: SP<br />state: SP","date: 2020-03-21<br />confirmed:  396<br />state: SP<br />state: SP","date: 2020-03-22<br />confirmed:  631<br />state: SP<br />state: SP","date: 2020-03-23<br />confirmed:  745<br />state: SP<br />state: SP","date: 2020-03-24<br />confirmed:  810<br />state: SP<br />state: SP","date: 2020-03-25<br />confirmed:  862<br />state: SP<br />state: SP","date: 2020-03-26<br />confirmed: 1052<br />state: SP<br />state: SP","date: 2020-03-27<br />confirmed: 1223<br />state: SP<br />state: SP","date: 2020-03-28<br />confirmed: 1406<br />state: SP<br />state: SP","date: 2020-03-29<br />confirmed: 1451<br />state: SP<br />state: SP","date: 2020-03-30<br />confirmed: 1517<br />state: SP<br />state: SP","date: 2020-03-31<br />confirmed: 2339<br />state: SP<br />state: SP","date: 2020-04-01<br />confirmed: 2981<br />state: SP<br />state: SP"],"type":"scatter","mode":"lines","line":{"width":1.88976377952756,"color":"rgba(253,231,37,1)","dash":"solid"},"hoveron":"points","name":"SP","legendgroup":"SP","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null}],"layout":{"margin":{"t":26.2283105022831,"r":7.30593607305936,"b":40.1826484018265,"l":48.9497716894977},"plot_bgcolor":"rgba(255,255,255,1)","paper_bgcolor":"rgba(255,255,255,1)","font":{"color":"rgba(0,0,0,1)","family":"","size":14.6118721461187},"xaxis":{"domain":[0,1],"automargin":true,"type":"linear","autorange":false,"range":[18316.25,18354.75],"tickmode":"array","ticktext":["Feb 24","Mar 02","Mar 09","Mar 16","Mar 23","Mar 30","Apr 06"],"tickvals":[18323,18330,18337,18344,18351],"categoryorder":"array","categoryarray":["Feb 24","Mar 02","Mar 09","Mar 16","Mar 23","Mar 30","Apr 06"],"nticks":null,"ticks":"outside","tickcolor":"rgba(51,51,51,1)","ticklen":3.65296803652968,"tickwidth":0.66417600664176,"showticklabels":true,"tickfont":{"color":"rgba(77,77,77,1)","family":"","size":11.689497716895},"tickangle":-0,"showline":false,"linecolor":null,"linewidth":0,"showgrid":true,"gridcolor":"rgba(235,235,235,1)","gridwidth":0.66417600664176,"zeroline":false,"anchor":"y","title":{"text":"Data","font":{"color":"rgba(0,0,0,1)","family":"","size":14.6118721461187}},"hoverformat":".2f"},"yaxis":{"domain":[0,1],"automargin":true,"type":"linear","autorange":false,"range":[-148,3130],"tickmode":"array","ticktext":["0","1000","2000","3000"],"tickvals":[0,1000,2000,3000],"categoryorder":"array","categoryarray":["0","1000","2000","3000"],"nticks":null,"ticks":"outside","tickcolor":"rgba(51,51,51,1)","ticklen":3.65296803652968,"tickwidth":0.66417600664176,"showticklabels":true,"tickfont":{"color":"rgba(77,77,77,1)","family":"","size":11.689497716895},"tickangle":-0,"showline":false,"linecolor":null,"linewidth":0,"showgrid":true,"gridcolor":"rgba(235,235,235,1)","gridwidth":0.66417600664176,"zeroline":false,"anchor":"x","title":{"text":"Casos Confirmados","font":{"color":"rgba(0,0,0,1)","family":"","size":14.6118721461187}},"hoverformat":".2f"},"shapes":[{"type":"rect","fillcolor":"transparent","line":{"color":"rgba(51,51,51,1)","width":0.66417600664176,"linetype":"solid"},"yref":"paper","xref":"paper","x0":0,"x1":1,"y0":0,"y1":1}],"showlegend":true,"legend":{"bgcolor":"rgba(255,255,255,1)","bordercolor":"transparent","borderwidth":1.88976377952756,"font":{"color":"rgba(0,0,0,1)","family":"","size":11.689497716895},"y":0.913385826771654},"annotations":[{"text":"Estado","x":1.02,"y":1,"showarrow":false,"ax":0,"ay":0,"font":{"color":"rgba(0,0,0,1)","family":"","size":14.6118721461187},"xref":"paper","yref":"paper","textangle":-0,"xanchor":"left","yanchor":"bottom","legendTitle":true}],"hovermode":"closest","barmode":"relative"},"config":{"doubleClick":"reset","showSendToCloud":false},"source":"A","attrs":{"db6f6a76ffe8":{"x":{},"y":{},"colour":{},"type":"scatter"}},"cur_data":"db6f6a76ffe8","visdat":{"db6f6a76ffe8":["function (y) ","x"]},"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.2,"selected":{"opacity":1},"debounce":0},"shinyEvents":["plotly_hover","plotly_click","plotly_selected","plotly_relayout","plotly_brushed","plotly_brushing","plotly_clickannotation","plotly_doubleclick","plotly_deselect","plotly_afterplot","plotly_sunburstclick"],"base_url":"https://plot.ly"},"evals":[],"jsHooks":[]}</script>
<p>Note que o gráfico acima é interativo. É possível obter mais informações sobre as curvas passando o mouse por cima delas, dando zoom na imagem e clicando nos nomes dos estados para desabilitá-los.</p>
<p>A seguir, faço um mapa com a taxa de casos confirmados para cada 100.000 habitantes:</p>
<pre class="r"><code>######################################
### analise exploratoria dos dados ###
######################################

# adiciona uma coluna chamada `estado`, com o nome do estado por extenso

codigos &lt;- structure(
  list(codigo = c(11L, 12L, 13L, 14L, 15L, 16L, 17L, 
                  21L, 22L, 23L, 24L, 25L, 26L, 27L, 
                  28L, 29L, 31L, 32L, 33L, 35L, 
                  41L, 42L, 43L, 50L, 51L, 52L, 53L), 
      estado = structure(c(22L, 1L, 4L, 23L, 14L, 3L, 
               27L, 10L, 18L, 6L, 20L, 15L, 17L, 2L, 26L, 
               5L, 13L, 8L, 19L, 25L, 16L, 24L, 21L, 12L, 
               11L, 9L, 7L), 
        .Label = c(&quot;Acre&quot;, &quot;Alagoas&quot;, &quot;Amapá&quot;, &quot;Amazonas&quot;, &quot;Bahia&quot;, 
                 &quot;Ceará&quot;, &quot;Distrito Federal&quot;, &quot;Espírito Santo&quot;, 
                 &quot;Goiás&quot;, &quot;Maranhão&quot;, &quot;Mato Grosso&quot;, 
                 &quot;Mato Grosso do Sul&quot;, &quot;Minas Gerais&quot;, &quot;Pará&quot;, 
                 &quot;Paraíba&quot;, &quot;Paraná&quot;, &quot;Pernambuco&quot;, 
                 &quot;Piauí&quot;, &quot;Rio de Janeiro&quot;, &quot;Rio Grande do Norte&quot;, 
                 &quot;Rio Grande do Sul&quot;, &quot;Rondônia&quot;, &quot;Roraima&quot;, 
                 &quot;Santa Catarina&quot;, &quot;São Paulo&quot;, &quot;Sergipe&quot;, 
                 &quot;Tocantins&quot;), class = &quot;factor&quot;), 
      uf = structure(c(21L, 1L, 3L, 22L, 14L, 4L, 27L, 10L, 
            17L, 6L, 20L, 15L, 16L, 2L, 25L, 5L, 
            11L, 8L, 19L, 26L, 18L, 24L, 23L, 12L, 13L, 9L, 7L), 
        .Label = c(&quot;AC&quot;, &quot;AL&quot;, &quot;AM&quot;, &quot;AP&quot;, &quot;BA&quot;, &quot;CE&quot;, &quot;DF&quot;, 
          &quot;ES&quot;, &quot;GO&quot;, &quot;MA&quot;, &quot;MG&quot;, &quot;MS&quot;, &quot;MT&quot;, &quot;PA&quot;, &quot;PB&quot;, 
          &quot;PE&quot;, &quot;PI&quot;, &quot;PR&quot;, &quot;RJ&quot;, &quot;RN&quot;, &quot;RO&quot;, &quot;RR&quot;, &quot;RS&quot;, 
          &quot;SC&quot;, &quot;SE&quot;, &quot;SP&quot;, &quot;TO&quot;), class = &quot;factor&quot;)), 
  class = &quot;data.frame&quot;, row.names = c(NA, -27L))</code></pre>
<pre class="r"><code>mapa_br &lt;- get_brmap(&quot;State&quot;)

mapa_br %&gt;%
  left_join(codigos, c(&quot;State&quot; = &quot;codigo&quot;)) %&gt;%
  left_join(filter(corona, date == hoje), c(&quot;uf&quot; = &quot;state&quot;)) %&gt;%
  ggplot() +
  geom_sf(aes(fill = confirmed_per_100k_inhabitants)) +
  labs(fill = &quot;Casos por 100k habitantes&quot;) +
  scale_fill_viridis_c(na.value = &quot;grey&quot;) +
  theme(panel.grid = element_line(colour = &quot;transparent&quot;),
        panel.background = element_blank(),
        axis.text = element_blank(),
        axis.ticks = element_blank(),
        rect = element_blank(),
        legend.position = &quot;bottom&quot;)</code></pre>
<p><a href="/images/mapa_corona.png">Clique para ver o Mapa com Taxa de Casos Confirmados de Coronavírus no Brasil</a></p>
<p>O que acho mais interessante na imagem acima é o destaque para o Distrito Federal. Note como ele possui uma cor muito diferente do restante do país, destacando-se muito em relação aos demais. Eu tenho algumas hipóteses para isso:</p>
<ol style="list-style-type: decimal">
<li>É o local mais importante da política nacional, então é natural que mais testes tenham sido realizados lá</li>
<li>Há um intenso fluxo de pessoas no DF</li>
<li>Com a renda per capita mais alta do país, é natural que mais pessoas consigam se testar por lá</li>
</ol>
<p>Alguém possui mais alguma hipótese para esta diferença tão grande entre o DF e os demais estados brasileiros?</p>
<p>Como sempre, o código utilizado nesta análise está disponível em meu <a href="https://github.com/mnunes/corona">github</a>.</p>
</div>
]]></content>
		</item>
		
		<item>
			<title>Algumas das piores visualizações de dados já feitas na história das eleições brasileiras e mundiais</title>
			<link>https://marcusnunes.me/posts/algumas-das-piores-visualizacoes-de-dados-ja-feitas/</link>
			<pubDate>Thu, 27 Feb 2020 07:57:00 -0300</pubDate>
			
			<guid>https://marcusnunes.me/posts/algumas-das-piores-visualizacoes-de-dados-ja-feitas/</guid>
			<description>2020 é ano de eleição. O que não falta nesta época do ano são eleitores, veículos de imprensa e assessorias de candidatos tentam o possível (e, muitas vezes, o impossível) para mostrar que seus representantes são os melhores. Meu objetivo ao criar este post foi mostrar como é possível mascarar dados reais e mentir com visualizações erradas.
Presidência da Venezuela, 2013 Um belo exemplo de como exagerar na diferença do percentual de votos entre dois candidatos a presidente, levando o leitor a acreditar que ela é muito maior do que realmente é.</description>
			<content type="html"><![CDATA[

<p>2020 é ano de eleição. O que não falta nesta época do ano são eleitores, veículos de imprensa e assessorias de candidatos tentam o possível (e, muitas vezes, o impossível) para mostrar que seus representantes são os melhores. Meu objetivo ao criar este post foi mostrar como é possível mascarar dados reais e mentir com visualizações erradas.</p>

<h2 id="presidência-da-venezuela-2013">Presidência da Venezuela, 2013</h2>

<p><img src="/images/maduro.jpg" alt="Presidência da Venezuela, 2013" /></p>

<p>Um belo exemplo de como exagerar na diferença do percentual de votos entre dois candidatos a presidente, levando o leitor a acreditar que ela é muito maior do que realmente é.</p>

<h2 id="presidência-do-brasil-2018">Presidência do Brasil, 2018</h2>

<p><img src="/images/ipof.jpg" alt="Presidência do Brasil, 2018" /></p>

<p>Detalhe para quem se responsabiliza por esta pesquisa: Instituto de Pesquisa Oficial do Face. Sim, <em>Face</em>, não Facebook. Exemplo clássico de viés de seleção em pesquisa, pois provavelmente aqueles que votaram nela são apenas os que concordam com determinada</p>

<h2 id="prefeitura-do-rio-de-janeiro-2016">Prefeitura do Rio de Janeiro, 2016</h2>

<p><img src="/images/crivella_jandira.jpg" alt="Prefeitura do Rio de Janeiro, 2016" /></p>

<p>Além da diferença entre os dois candidatos não estar proporcional, veja como um ponto percentual faz a intenção de votos para Jandira Feghali subir mais intensamente do que os dois pontos percentuais de Marcelo Crivella fazem por sua intenção de voto.</p>

<h2 id="prefeitura-de-porto-alegre-2016">Prefeitura de Porto Alegre, 2016</h2>

<p>À primeira vista, somos levados a acreditar que Sebastião Melo é o líder da pesquisa eleitoral na cidade. Entretanto, ele estava em terceiro lugar no momento em que este gráfico foi divulgado.</p>

<p><img src="/images/melo.png" alt="Presidência do Brasil, 2018" /></p>

<h2 id="governo-de-são-paulo-2018">Governo de São Paulo, 2018</h2>

<p><img src="/images/doria_lidera.jpg" alt="Governo de São Paulo, 2018" /></p>

<p>Este é o meu gráfico preferido, pois não tem quase nada certo nele:</p>

<ul>
<li>os 22% do Doria estão próximos da escala de 90% do gráfico</li>
<li>graficamente, os 22% de Doria são mais de 4 vezes maiores do que os 15% de Skaf</li>
<li>além diss, estes mesmos 22% tem aproximadamente o dobro do tamanho dos 40% do percentual de indecisos</li>
<li>todos os outros candidatos estão desalinhados em relação às linhas de referência do gráfico</li>
<li>o símbolo de &ldquo;avança&rdquo;, feito pelo candidato na foto com a sua mão direita, aponta para a esquerda, que é a direção normalmente associada ao retrecesso</li>
</ul>

<h2 id="bônus-futebol">Bônus: Futebol</h2>

<p>Não conheço ninguém que tenha conseguido interpretar o gráfico abaixo em menos de 30 segundos. Tem gente inclusive que demora muito mais de um minuto para isso. Tente entender o que está representado abaixo e deixe a sua resposta nos comentários.</p>

<p><img src="/images/gols.jpg" alt="Bônus: Futebol" /></p>
]]></content>
		</item>
		
		<item>
			<title>Heatmap com os dados de geolocalização do Google Location History: minha viagem pela Itália</title>
			<link>https://marcusnunes.me/posts/heatmap-de-localizacao-no-google/</link>
			<pubDate>Mon, 27 Jan 2020 15:14:00 -0300</pubDate>
			
			<guid>https://marcusnunes.me/posts/heatmap-de-localizacao-no-google/</guid>
			<description>Uma das coisas que mais gosto na modernidade são os serviços de localização. Sim, eu sei que a minha suposta privacidade é completamente perdida quando compartilho meus dados de localização o tempo todo, mas acredito que as facilidades que isso me traz superam, e muito, as desvantagens. O problema é que eu não sabia muito o que fazer com estes dados.
Mas eis que um usuário do github chamado luka1199 criou o repositório Generate an interactive geo heatmap from your Google location data para auxiliar na geração de heatmaps a partir dos dados de localização dos usuários.</description>
			<content type="html"><![CDATA[<p>Uma das coisas que mais gosto na modernidade são os serviços de localização. Sim, eu sei que a minha suposta privacidade é completamente perdida quando compartilho meus dados de localização o tempo todo, mas acredito que as facilidades que isso me traz superam, e muito, as desvantagens. O problema é que eu não sabia muito o que fazer com estes dados.</p>

<p>Mas eis que um usuário do github chamado <a href="https://github.com/luka1199/" target="_blank">luka1199</a> criou o repositório <a href="https://github.com/luka1199/geo-heatmap" target="_blank">Generate an interactive geo heatmap from your Google location data</a> para auxiliar na geração de heatmaps a partir dos dados de localização dos usuários. Ele permite que qualquer pessoa baixe seus dados do Google e gere, com muito pouco trabalho, visualizações interessantes sobre seus hábitos de movimentação.</p>

<p>Por exemplo, este é o heatmap que gerei a partir das minhas férias na Itália e Portugal, entre Dezembro de 2019 e Janeiro de 2020. Comecei em Veneza e de lá desci para Florença, Roma, Nápoles, Catânia, Siracusa, Agrigento e Palermo, para depois seguir para Lisboa e Sintra, em Portugal.</p>

<p><img src="/images/italia.png" alt="Mapa da Itália" /></p>

<p><img src="/images/portugal.png" alt="Mapa de Portugal" /></p>

<p>Logicamente, estas são imagens estáticas, mas ao final do post eu coloquei o mapa interativo para vocês verem como ele funciona na prática.</p>

<p>Para quem já tem um pouco de experiência com <a href="https://marcusnunes.me/tags/python" target="_blank">python</a>, o script é muito fácil de usar:</p>

<ol>
<li>Instale o <a href="https://www.python.org/downloads/" target="_blank">python 3+</a> em seu computador</li>
<li>Baixe seus dados de localização a partir do <a href="https://takeout.google.com/" target="_blank">Google Takeout</a></li>
<li>Clone o <a href="https://github.com/luka1199/geo-heatmap" target="_blank">repositório</a> com os scripts necessários em sua máquina local ou na nuvem</li>
<li>Rode o comando <code>pip install -r requirements.txt</code> na pasta com o repositório clonado</li>
<li>Por fim, execute <code>python geo_heatmap.py NomeDoArquivo.json</code> para gerar o seu mapa interativo</li>
</ol>

<p>O meu heatmap interativo pode ser visualizado abaixo:</p>

<iframe src="/images/heatmap.html" width=100% height=600></iframe>

<p>Comente aqui embaixo o que você achou dessa ferramenta. Ou, caso não tenha conseguido usá-la, relate as suas dificuldades.</p>

<p>Aproveite que chegou até aqui e me siga no <a href="https://instagram.com/grandeabobora/" target="_blank">instagram</a> para ver as fotos que estou publicando a respeito dessa viagem.</p>



<blockquote class="instagram-media" data-instgrm-captioned data-instgrm-permalink="https://www.instagram.com/p/B625CmDBMHU/?utm_source=ig_embed&amp;utm_campaign=loading" data-instgrm-version="12" style=" background:#FFF; border:0; border-radius:3px; box-shadow:0 0 1px 0 rgba(0,0,0,0.5),0 1px 10px 0 rgba(0,0,0,0.15); margin: 1px; max-width:658px; min-width:326px; padding:0; width:99.375%; width:-webkit-calc(100% - 2px); width:calc(100% - 2px);"><div style="padding:16px;"> <a href="https://www.instagram.com/p/B625CmDBMHU/?utm_source=ig_embed&amp;utm_campaign=loading" style=" background:#FFFFFF; line-height:0; padding:0 0; text-align:center; text-decoration:none; width:100%;" target="_blank"> <div style=" display: flex; flex-direction: row; align-items: center;"> <div style="background-color: #F4F4F4; border-radius: 50%; flex-grow: 0; height: 40px; margin-right: 14px; width: 40px;"></div> <div style="display: flex; flex-direction: column; flex-grow: 1; justify-content: center;"> <div style=" background-color: #F4F4F4; border-radius: 4px; flex-grow: 0; height: 14px; margin-bottom: 6px; width: 100px;"></div> <div style=" background-color: #F4F4F4; border-radius: 4px; flex-grow: 0; height: 14px; width: 60px;"></div></div></div><div style="padding: 19% 0;"></div> <div style="display:block; height:50px; margin:0 auto 12px; width:50px;"><svg width="50px" height="50px" viewBox="0 0 60 60" version="1.1" xmlns="https://www.w3.org/2000/svg" xmlns:xlink="https://www.w3.org/1999/xlink"><g stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"><g transform="translate(-511.000000, -20.000000)" fill="#000000"><g><path d="M556.869,30.41 C554.814,30.41 553.148,32.076 553.148,34.131 C553.148,36.186 554.814,37.852 556.869,37.852 C558.924,37.852 560.59,36.186 560.59,34.131 C560.59,32.076 558.924,30.41 556.869,30.41 M541,60.657 C535.114,60.657 530.342,55.887 530.342,50 C530.342,44.114 535.114,39.342 541,39.342 C546.887,39.342 551.658,44.114 551.658,50 C551.658,55.887 546.887,60.657 541,60.657 M541,33.886 C532.1,33.886 524.886,41.1 524.886,50 C524.886,58.899 532.1,66.113 541,66.113 C549.9,66.113 557.115,58.899 557.115,50 C557.115,41.1 549.9,33.886 541,33.886 M565.378,62.101 C565.244,65.022 564.756,66.606 564.346,67.663 C563.803,69.06 563.154,70.057 562.106,71.106 C561.058,72.155 560.06,72.803 558.662,73.347 C557.607,73.757 556.021,74.244 553.102,74.378 C549.944,74.521 548.997,74.552 541,74.552 C533.003,74.552 532.056,74.521 528.898,74.378 C525.979,74.244 524.393,73.757 523.338,73.347 C521.94,72.803 520.942,72.155 519.894,71.106 C518.846,70.057 518.197,69.06 517.654,67.663 C517.244,66.606 516.755,65.022 516.623,62.101 C516.479,58.943 516.448,57.996 516.448,50 C516.448,42.003 516.479,41.056 516.623,37.899 C516.755,34.978 517.244,33.391 517.654,32.338 C518.197,30.938 518.846,29.942 519.894,28.894 C520.942,27.846 521.94,27.196 523.338,26.654 C524.393,26.244 525.979,25.756 528.898,25.623 C532.057,25.479 533.004,25.448 541,25.448 C548.997,25.448 549.943,25.479 553.102,25.623 C556.021,25.756 557.607,26.244 558.662,26.654 C560.06,27.196 561.058,27.846 562.106,28.894 C563.154,29.942 563.803,30.938 564.346,32.338 C564.756,33.391 565.244,34.978 565.378,37.899 C565.522,41.056 565.552,42.003 565.552,50 C565.552,57.996 565.522,58.943 565.378,62.101 M570.82,37.631 C570.674,34.438 570.167,32.258 569.425,30.349 C568.659,28.377 567.633,26.702 565.965,25.035 C564.297,23.368 562.623,22.342 560.652,21.575 C558.743,20.834 556.562,20.326 553.369,20.18 C550.169,20.033 549.148,20 541,20 C532.853,20 531.831,20.033 528.631,20.18 C525.438,20.326 523.257,20.834 521.349,21.575 C519.376,22.342 517.703,23.368 516.035,25.035 C514.368,26.702 513.342,28.377 512.574,30.349 C511.834,32.258 511.326,34.438 511.181,37.631 C511.035,40.831 511,41.851 511,50 C511,58.147 511.035,59.17 511.181,62.369 C511.326,65.562 511.834,67.743 512.574,69.651 C513.342,71.625 514.368,73.296 516.035,74.965 C517.703,76.634 519.376,77.658 521.349,78.425 C523.257,79.167 525.438,79.673 528.631,79.82 C531.831,79.965 532.853,80.001 541,80.001 C549.148,80.001 550.169,79.965 553.369,79.82 C556.562,79.673 558.743,79.167 560.652,78.425 C562.623,77.658 564.297,76.634 565.965,74.965 C567.633,73.296 568.659,71.625 569.425,69.651 C570.167,67.743 570.674,65.562 570.82,62.369 C570.966,59.17 571,58.147 571,50 C571,41.851 570.966,40.831 570.82,37.631"></path></g></g></g></svg></div><div style="padding-top: 8px;"> <div style=" color:#3897f0; font-family:Arial,sans-serif; font-size:14px; font-style:normal; font-weight:550; line-height:18px;"> View this post on Instagram</div></div><div style="padding: 12.5% 0;"></div> <div style="display: flex; flex-direction: row; margin-bottom: 14px; align-items: center;"><div> <div style="background-color: #F4F4F4; border-radius: 50%; height: 12.5px; width: 12.5px; transform: translateX(0px) translateY(7px);"></div> <div style="background-color: #F4F4F4; height: 12.5px; transform: rotate(-45deg) translateX(3px) translateY(1px); width: 12.5px; flex-grow: 0; margin-right: 14px; margin-left: 2px;"></div> <div style="background-color: #F4F4F4; border-radius: 50%; height: 12.5px; width: 12.5px; transform: translateX(9px) translateY(-18px);"></div></div><div style="margin-left: 8px;"> <div style=" background-color: #F4F4F4; border-radius: 50%; flex-grow: 0; height: 20px; width: 20px;"></div> <div style=" width: 0; height: 0; border-top: 2px solid transparent; border-left: 6px solid #f4f4f4; border-bottom: 2px solid transparent; transform: translateX(16px) translateY(-4px) rotate(30deg)"></div></div><div style="margin-left: auto;"> <div style=" width: 0px; border-top: 8px solid #F4F4F4; border-right: 8px solid transparent; transform: translateY(16px);"></div> <div style=" background-color: #F4F4F4; flex-grow: 0; height: 12px; width: 16px; transform: translateY(-4px);"></div> <div style=" width: 0; height: 0; border-top: 8px solid #F4F4F4; border-left: 8px solid transparent; transform: translateY(-4px) translateX(8px);"></div></div></div></a> <p style=" margin:8px 0 0 0; padding:0 4px;"> <a href="https://www.instagram.com/p/B625CmDBMHU/?utm_source=ig_embed&amp;utm_campaign=loading" style=" color:#000; font-family:Arial,sans-serif; font-size:14px; font-style:normal; font-weight:normal; line-height:17px; text-decoration:none; word-wrap:break-word;" target="_blank">Que rua molhada essa aí #venice #veneza #italia #ferias #canoneosrp #canon24105rf #paisagem #city #streetphotography #street #afternoon</a></p> <p style=" color:#c9c8cd; font-family:Arial,sans-serif; font-size:14px; line-height:17px; margin-bottom:0; margin-top:8px; overflow:hidden; padding:8px 0 7px; text-align:center; text-overflow:ellipsis; white-space:nowrap;">A post shared by <a href="https://www.instagram.com/grandeabobora/?utm_source=ig_embed&amp;utm_campaign=loading" style=" color:#c9c8cd; font-family:Arial,sans-serif; font-size:14px; font-style:normal; font-weight:normal; line-height:17px;" target="_blank"> Marcus Nunes</a> (@grandeabobora) on <time style=" font-family:Arial,sans-serif; font-size:14px; line-height:17px;" datetime="2020-01-03T12:34:31+00:00">Jan 3, 2020 at 4:34am PST</time></p></div></blockquote>
<script async src="//www.instagram.com/embed.js"></script>
]]></content>
		</item>
		
		<item>
			<title>Analise os dados abertos da ANAC</title>
			<link>https://marcusnunes.me/posts/dados-coletados-da-anac/</link>
			<pubDate>Thu, 26 Dec 2019 07:36:00 -0300</pubDate>
			
			<guid>https://marcusnunes.me/posts/dados-coletados-da-anac/</guid>
			<description>Organizei todos os registros de voos do site da ANAC em um pacote do R. São mais de 20 milhões de informações referentes a todos os voos civis registrados no Brasil desde 2000, com as seguintes variáveis:
 ICAOEmpresaAerea - Código designador da empresa no padrão da ICAO, com três letras. NumeroVoo - Numeração do voo, com até 4 caracteres numéricos. CodigoAutorizacaoDI - Caractere utilizado para identificar o tipo de autorização para cada etapa de voo conforme IAC 1504, podendo ser 0 (voo regular), 1 (voo extra com HOTRAN), 2 (voo extra sem HOTRAN), 3 (voo de retorno), 4 (inclusão de etapa em um voo previsto em HOTRAN), 5 (voo cargueiro), 6 (voo de serviço), 7 (voo de fretamento), 9 (voo charter), A (voo de instrução) ou B (voo de experiência).</description>
			<content type="html"><![CDATA[<p>Organizei todos os registros de voos do site da ANAC em um pacote do R. São mais de 20 milhões de informações referentes a todos os voos civis registrados no Brasil desde 2000, com as seguintes variáveis:</p>

<ul>
<li><code>ICAOEmpresaAerea</code> - Código designador da empresa no padrão da ICAO, com três letras.</li>
<li><code>NumeroVoo</code> - Numeração do voo, com até 4 caracteres numéricos.</li>
<li><code>CodigoAutorizacaoDI</code> - Caractere utilizado para identificar o tipo de autorização para cada etapa de voo conforme IAC 1504, podendo ser 0 (voo regular), 1 (voo extra com HOTRAN), 2 (voo extra sem HOTRAN), 3 (voo de retorno), 4 (inclusão de etapa em um voo previsto em HOTRAN), 5 (voo cargueiro), 6 (voo de serviço), 7 (voo de fretamento), 9 (voo charter), A (voo de instrução) ou B (voo de experiência).</li>
<li><code>CodigoTipoLinha</code> - Identifica o tipo de operação realizada, conforme IAC 1504, sendo N (nacional), I (internacional), R (regional), H (sub-regional), E (especial), C (cargueiro), G (cargueiro internacional) e L (rede postal).</li>
<li><code>ICAOAerodromoOrigem</code> - Código ICAO do aeroporto de origem.</li>
<li><code>ICAOAerodromoDestino</code> - Código ICAO do aeroporto de destino.</li>
<li><code>PartidaPrevista</code> - Data e horário da partida prevista em horário de Brasília.</li>
<li><code>PartidaReal</code> - Data e horário da partida realizada informada pela empresa aérea, em horário de Brasília.</li>
<li><code>ChegadaPrevista</code> - Data e horário da chegada prevista em horário de Brasília.</li>
<li><code>ChegadaReal</code> - Data e horário da chegada realizada, informada pela empresa aérea, em horário de Brasília.</li>
<li><code>SituacaoVoo</code> - Campo informando se o voo foi realizado ou cancelado.</li>
<li><code>CodigoJustificativa</code> - Identifica o motivo do atraso, cancelamento e demais alterações em relação ao voo planejado, conforme IAC 1504.</li>
</ul>

<p><a href="https://github.com/mnunes/anac/" target="_blank">Visite meu github</a> para encontrar o link para baixar o pacote e as suas instruções de uso.</p>
]]></content>
		</item>
		
		<item>
			<title>Como simplificar seus relatórios estatísticos usando R Markdown</title>
			<link>https://marcusnunes.me/posts/como-simplificar-seus-relato%CC%81rios-estati%CC%81sticos-usando-r-markdown/</link>
			<pubDate>Mon, 18 Nov 2019 21:55:00 -0300</pubDate>
			
			<guid>https://marcusnunes.me/posts/como-simplificar-seus-relato%CC%81rios-estati%CC%81sticos-usando-r-markdown/</guid>
			<description>Hoje apresentei mais um minicurso. A organização da 2nd Conference on Statistics and Data Science me chamou para contribuir com um assunto referente à ciência de dados no evento. Escolhi falar sobre R Markdown, a tecnologia que utilizamos para escrever os relatórios de consultoria do Laboratório de Estatística Aplicada. Inclusive, temos um pacote no R chamado modeloLEA, escrito para agilizar e padronizar a formatação destes nossos relatórios.
O material do minicurso pode ser acessado em github.</description>
			<content type="html"><![CDATA[<p>Hoje apresentei mais um minicurso. A organização da <a href="http://www.csds2019.ime.ufba.br/" target="_blank">2nd Conference on Statistics and Data Science</a> me chamou para contribuir com um assunto referente à ciência de dados no evento. Escolhi falar sobre <a href="https://marcusnunes.me/tags/r%20markdown/" target="_blank">R Markdown</a>, a tecnologia que utilizamos para escrever os relatórios de consultoria do <a href="http://lea.estatistica.ccet.ufrn.br/" target="_blank">Laboratório de Estatística Aplicada</a>. Inclusive, temos um pacote no R chamado <a href="https://github.com/mnunes/modeloLEA/" target="_blank">modeloLEA</a>, escrito para agilizar e padronizar a formatação destes nossos relatórios.</p>

<p><img src="/images/IMG_2410_mini.jpg" alt="Apresentação do minicurso na 2nd Conference on Statistics and Data Science" /></p>

<p>O material do minicurso pode ser acessado em <a href="https://github.com/mnunes/2nd-csds" target="_blank">github.com/mnunes/2nd-csds</a>. Como sempre, o conteúdo disponibilizado é gratuito. Como amostra do trabalho, fiquem com o pdf que possui a <a href="https://github.com/mnunes/2nd-csds/blob/master/presentation/presentation.pdf" target="_blank">apresentação de slides</a> utilizada durante meu minicurso.</p>

<p>Se estiver interessado em uma versão deste minicurso para o seu evento, <a href="https://marcusnunes.me/contato/" target="_blank">entre em contato</a> para que possamos conversar sobre uma possível colaboração.</p>
]]></content>
		</item>
		
		<item>
			<title>Resultados da Competição de Ciência de Dados da UFRN 2019</title>
			<link>https://marcusnunes.me/posts/resultados-da-competicao-de-ciencia-de-dados-da-ufrn-2019/</link>
			<pubDate>Thu, 31 Oct 2019 19:03:00 -0300</pubDate>
			
			<guid>https://marcusnunes.me/posts/resultados-da-competicao-de-ciencia-de-dados-da-ufrn-2019/</guid>
			<description>Conforme eu havia avisado anteriormente, o Departamento de Estatística da UFRN promoveu mais um vez a sua tradicional competição de Ciência de Dados entre os alunos da universidade.
Desta vez, a banca avaliadora não contava apenas com professores do Departamento de Estatística como na competição de 2018. No centro da foto abaixo, que contempla todos os competidores, é possível ver lado a lado Leonardo Bezerra, professor do IMD, Carla Vivacqua e eu, professores do Departamento de Estatística.</description>
			<content type="html"><![CDATA[

<p><a href="https://marcusnunes.me/posts/competicao-de-ciencia-de-dados-da-ufrn-2019/" target="_blank">Conforme eu havia avisado anteriormente</a>, o Departamento de Estatística da UFRN promoveu mais um vez a sua tradicional competição de Ciência de Dados entre os alunos da universidade.</p>

<p><img src="/images/ccd_2019.png" alt="Logo da Competição de Ciência de Dados da UFRN 2019" /></p>

<p>Desta vez, a banca avaliadora não contava apenas com professores do Departamento de Estatística como na <a href="https://marcusnunes.me/posts/como-foi-a-competicao-de-ciencia-de-dados/" target="_blank">competição de 2018</a>. No centro da foto abaixo, que contempla todos os competidores, é possível ver lado a lado Leonardo Bezerra, professor do IMD, Carla Vivacqua e eu, professores do Departamento de Estatística.</p>

<p><img src="/images/IMG_2195.jpg" alt="Competidores presentes na edição de 2019" /></p>

<p>Foram 21 competidores divididos em 7 equipes. O desafio era os dados coletados pelo INMET - Instituto Nacional de Meteorologia. Estes dados podem ser baixados a partir do endereço <a href="https://www.dropbox.com/s/7rriacb7c6vzf3m/ccd_2019.zip?dl=0" target="_blank">https://www.dropbox.com/s/7rriacb7c6vzf3m/ccd_2019.zip?dl=0</a> . O arquivo zipado tem 214MB. Eles foram obtidos com a ajuda de um pacote do R chamado inmetr - <a href="https://github.com/jdtatsch/inmetr" target="_blank">https://github.com/jdtatsch/inmetr</a> - , criado pelo Professor Jonatan Tatsch, da Universidade Federal de Santa Maria.</p>

<p>São dois arquivos dentro do zip. O arquivo <code>inmetr.csv</code> possui informações sobre as seguintes variáveis:</p>

<pre><code>variável                                descrição       unidade
    date                     data e hora da coleta             -
      id                   ID da estação de coleta             -
    prec                              precipitação            mm
    tair                         temperatura do ar graus Celsius
      tw                temperatura de bulbo úmido graus Celsius
    tmax                  temperatura máxima do ar graus Celsius
    tmin                  temperatura mínima do ar graus Celsius
   urmax                   umidade relativa máxima             %
    patm                       pressão atmosférica           hPa
    pnmm pressão atmosférica média ao nível do mar           hPa
      wd                          direção do vento         graus
   wsmax                          rajadas de vento           m/s
       n                              horas de sol             h
      cc                       cobertura de nuvens             -
    evap                                evaporação            mm
      ur                          umidade relativa             %
      ws                       velocidade do vento           m/s
</code></pre>

<p>O arquivo <code>bdmep_meta.csv</code> possui informações sobre as estações de coleta, relacionando-as com a variável id do arquivo <code>inmetr.csv</code>.</p>

<p>As equipes precisaram seguir algumas poucas regras gerais:</p>

<ul>
<li><p>O prazo limite para a entrega das análises era 23:59 de 27 de outubro de 2019.</p></li>

<li><p>As apresentações deveriam se limitar a 10 minutos e 3 slides</p></li>

<li><p>Qualquer análise era válida, desde visualizações dos dados até algum tipo de modelagem</p></li>
</ul>

<p>Após as apresentações, a banca decidiu que as 3 equipes melhores colocadas foram estas listadas abaixo, com as suas respectivas apresentações e relatórios linkados na sequência:</p>

<h2 id="3º-lugar-numberone">3º Lugar: numberOne</h2>

<p><img src="/images/IMG_2174.jpg" alt="Number One: Jordão de Lima Alves" /></p>

<p>Jordão de Lima Alves (Ciências Atuariais) criou a equipe de um homem só e faturou o terceiro lugar. Ele utilizou métodos de séries temporais para analisar alguns dados climatológicos da cidade de Apodi, no Rio Grande do Norte. Saiba mais detalhes sobre o trabalho dele lendo os <a href="/images/ciencia_de_dados_2019/numberOne_Apresentacao.pdf">slides</a> e o <a href="/images/ciencia_de_dados_2019/numberOne_Relatorio.pdf">relatório</a> entregues.</p>

<h2 id="2º-lugar-weeee">2º Lugar: weeee</h2>

<p><img src="/images/IMG_2178.jpg" alt="weeee: André Fellipe da Silva, Ianca Maria Leite da Costa, Vítor Saraiva Ramos, André Luis Andrade Machado e Mariana Costa" /></p>

<p>Os alunos André Fellipe da Silva (Graduação em Engenharia Elétrica), Ianca Maria Leite da Costa (Graduação em Engenharia Elétrica), Vítor Saraiva Ramos (Mestrado em Engenharia Elétrica e de Computação), André Luis Andrade Machado (Graduação em Engenharia Elétrica) e Mariana Costa (Graduação em Engenharia de Produção) se reuniram para analisar os dados meteorológicos disponíveis sobre 19/08/2019, o &ldquo;dia que virou noite&rdquo; devido à fumaça proveniente das queimadas na Amazônia. Baixe os <a href="/images/ciencia_de_dados_2019/weeee_Apresentacao.pdf">slides</a> e o <a href="/images/ciencia_de_dados_2019/weeee_Relatorio.pdf">relatório</a> para ver o que esta equipe produziu. Acesse a <a href="http://bit.ly/slides_wee" target="_blank">apresentação de slides na web</a> para ver uma animação que a equipe produziu para ilustrar os seus achados. As análises podem ser reproduzidas com o <a href="https://github.com/vitorsr/ccd" target="_blank">código em python</a> escrito pela equipe.</p>

<h2 id="1º-lugar-featuring-the-future">1º Lugar: Featuring the Future</h2>

<p><img src="/images/IMG_2185.jpg" alt="Number One: Daniel Marx Pinto Carvalho, Marcos Vinícius Gomes Jacinto, Gilvandro César de Medeiros e Marcus Vinicius Oliveira de Brito" /></p>

<p>Os grandes campeões do evento foram os alunos Daniel Marx Pinto Carvalho (Graduação em Tecnologia da Informação), Marcos Vinícius Gomes Jacinto (Programa de Pós-Graduação em Geodinâmica e Geofísica), Gilvandro César de Medeiros (Bacharelado em Ciências e Tecnologia) e Marcus Vinicius Oliveira de Brito (Graduação em Engenharia Elétrica). Como o conjunto de dados disponibilizado durante a competição possuía muitos dados faltantes, eles utilizaram <em>deep learning</em> para imputar as informações faltantes, além de analisar o potencial eólico das cidades brasileiras utilizando a distribuição Weibull. Seus achados estão disponíveis nos <a href="/images/ciencia_de_dados_2019/Featuring_the_Future_Slides.pdf">slides</a>, no <a href="/images/ciencia_de_dados_2019/Featuring_the_Future_Relatorio.pdf">relatório</a> e no <a href="https://github.com/gilvandrocesardemedeiros/CCD" target="_blank">código</a> fornecidos pela equipe.</p>
]]></content>
		</item>
		
		<item>
			<title>R ou Python? Qual a melhor ferramenta para trabalhar com Ciência de Dados?</title>
			<link>https://marcusnunes.me/posts/r-ou-python/</link>
			<pubDate>Wed, 09 Oct 2019 16:26:00 -0300</pubDate>
			
			<guid>https://marcusnunes.me/posts/r-ou-python/</guid>
			<description>Já vou começar este texto opinativo com a resposta definitiva para a pergunta do título:
Qual é a melhor ferramenta para Análise de Dados? A melhor ferramenta para análise dados é a ferramenta que o usuário mais conhece. Seja Excel, R, python, Minitab, SAS, SPSS ou qualquer outra. O que importa é obter os resultados desejados de maneira rápida e confiável. Cada usuário vai ter necessidades diferentes e é muito provável que a melhor ferramenta para uma pessoa não sirva para outra.</description>
			<content type="html"><![CDATA[

<p>Já vou começar este texto opinativo com a resposta definitiva para a pergunta do título:</p>

<h1 id="qual-é-a-melhor-ferramenta-para-análise-de-dados">Qual é a melhor ferramenta para Análise de Dados?</h1>

<p>A melhor ferramenta para análise dados é a ferramenta que o usuário mais conhece. Seja Excel, <a href="https://marcusnunes.me/tags/r/" target="_blank">R</a>, <a href="https://marcusnunes.me/tags/python/" target="_blank">python</a>, Minitab, SAS, SPSS ou qualquer outra. O que importa é obter os resultados desejados de maneira rápida e confiável. Cada usuário vai ter necessidades diferentes e é muito provável que a melhor ferramenta para uma pessoa não sirva para outra.</p>

<p>Isto posto, afirmo que a minha linguagem preferida de programação para análise de dados é o <a href="https://marcusnunes.me/tags/r/" target="_blank">R</a>. Ela não é a linguagem mais popular do mundo, nem a mais rápida e nem a mais simples de aprender. Entretanto, é a que <strong>me</strong> serve melhor para aquilo que <strong>eu</strong> faço.</p>

<!--No restante do texto vou comparar [R](https://marcusnunes.me/tags/r/) e [python](https://marcusnunes.me/tags/python/) dentro das minhas limitações e dos meus conhecimentos e dizer porque prefiro uma em relação à outra.-->

<h1 id="por-que-usar-uma-linguagem-de-programação-para-analisar-dados">Por que usar uma linguagem de programação para analisar dados?</h1>

<p>O principal motivo é a documentação do processo de análise de dados. De maneira geral, ao utilizar uma ferramenta como Excel ou SPSS, o usuário não documenta aquilo que faz. Ele clica nos menus, obtém o seu resultado e termina o seu serviço. Não há nada de errado em fazer isso. O problema é que análises mais complexas acabam gerando mais passos intermediários entre a importação de dados e o resultado final da análise. A imagem abaixo, adaptada do livro <a href="https://r4ds.had.co.nz/" target="_blank">R for Data Science</a>, exemplifica bem como é o workflow geral de uma análise de dados do início ao fim:</p>

<p><img src="/images/workshop.png" alt="" /></p>

<p>Perceba que há muitos passos envolvidos. O processo completo, que começa pela importação dos dados, passa pela modelagem e finaliza na comunicação dos resultados, envolve muitos passos diferentes. Em especial, a parte destacada pelo retângulo azul, pode demorar muito. Encontrar o modelo ideal para os dados não é uma tarefa trivial. Assim, documentar estes passos é fundamental não apenas para que consigamos organizar nossas ideias, mas também para informar os outros membros da nossa equipe a respeito do trabalho que realizamos.</p>

<h1 id="por-que-usar-o-r-para-analisar-dados">Por que usar o R para analisar dados?</h1>

<p>A figura abaixo, obtida no <a href="https://julialang.org/benchmarks/" target="_blank">site da linguagem julia</a>, compara a velocidade de diversas linguagens de programação:</p>

<p><img src="/images/benchmarks.png" alt="" /></p>

<p>Note que <a href="https://marcusnunes.me/tags/r/" target="_blank">R</a> não é a mais rápida em nenhuma tarefa. Mesmo assim, prefiro utilizá-la porque escrevo código mais rápido nesta linguagem. O ganho de performance que eu teria rodando código em <a href="https://marcusnunes.me/tags/python/" target="_blank">python</a> seria perdido na hora de escrever os programas, pois sempre tenho que ficar checando manuais quando uso python. Ocorre o oposto quando uso R, pois sou fluente na linguagem e escrevo código para ela como escrevo meus textos em português ou em inglês.</p>

<p>O <a href="https://marcusnunes.me/tags/r/" target="_blank">R</a> apresenta algumas outras facilidades que eu já não consigo viver sem. Como preparo muito material didático, esta linguagem permite que eu mescle conteúdo teórico, trechos de códigos e outputs de programas de maneira muito prática. Sério, dá uma olhada neste material do <a href="https://htmlpreview.github.io/?https://github.com/mnunes/workshopR/blob/master/inst/doc/workshopR.html" target="_blank">Workshop de R Básico</a> que ministro e veja como a qualidade gráfica é impressionante. Faz quase 10 anos que uso R Sweave e seu sucessor RMarkdown com muito sucesso e não tenho interesse em aprender uma nova ferramenta se as que conheço atualmente já me satisfazem a contento.</p>

<p>Acabei utilizando esta característica do <a href="https://marcusnunes.me/tags/r/" target="_blank">R</a> para criar aplicações do RMarkdown em outras áreas. Por exemplo, os <a href="https://github.com/mnunes/modeloLEA/blob/master/documento_final.pdf" target="_blank">relatórios</a> dos alunos que participam do meu projeto de <a href="http://marcusnunes.me/consultoria/" target="_blank">consultoria estatística gratuita</a> são escritos usando RMarkdown, através de um <a href="https://github.com/mnunes/modeloLEA" target="_blank">pacote que eu mesmo criei</a>.</p>

<h1 id="conclusão">Conclusão</h1>

<p>Eu uso <a href="https://marcusnunes.me/tags/r/" target="_blank">R</a> porque ele facilita a minha vida. Programo na linguagem desde 2008 e, antes disso, utilizei S-Plus, sua versão proprietária. Por isso, o <a href="https://marcusnunes.me/tags/r/" target="_blank">R</a> é a minha opção preferida para analisar dados, preparar material didático e escrever relatórios e artigos científicos.</p>
]]></content>
		</item>
		
	</channel>
</rss>
